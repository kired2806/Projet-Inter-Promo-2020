{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"g5_clustering_v1.ipynb","provenance":[{"file_id":"1maEGL4K55B_aTirI-59zw0xoYhOJGgNi","timestamp":1610893741795}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"e223ca94b3674ff0ac81d61f90567905":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54c0eb8b2ef944cdab33a8fc7af19da7","IPY_MODEL_720b7c62f279409f810526ed465c6da3"],"layout":"IPY_MODEL_c9d3834a7e1645fdac1a0d382b279f5f"}},"54c0eb8b2ef944cdab33a8fc7af19da7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"  0%","description_tooltip":null,"layout":"IPY_MODEL_da9bfbaf2f824d1c8dd21e032a948cdb","max":17,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b39ecc077ee545f6a297094a23d81b4d","value":0}},"720b7c62f279409f810526ed465c6da3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64695c6bad3e453e983a0ef0bad3d106","placeholder":"​","style":"IPY_MODEL_7e654e406f854ea7b33d25fcef820b79","value":" 0/17 [00:00&lt;?, ?it/s]"}},"c9d3834a7e1645fdac1a0d382b279f5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da9bfbaf2f824d1c8dd21e032a948cdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b39ecc077ee545f6a297094a23d81b4d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"64695c6bad3e453e983a0ef0bad3d106":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e654e406f854ea7b33d25fcef820b79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"RHjHS776JskZ"},"source":["Created on Friday 8 January 2021\n","\n","**Group 5 - Clustering**  \n","**Clustering v1**\n","\n","@authors : Theo Vedis, Jérémy Johann, Damien Izard, Nour Elhouda Kired, Paule Cadrelle Massag, Jessicka Mucy-Clavier, Gabriel nathir Kassem Rojas"]},{"cell_type":"markdown","metadata":{"id":"sdIzSPoh0a8B"},"source":["This code compute the dataframe containg all the article content and id and clusterizes them into an optimal number of clusters.\n","This number is based on the maximum value of the error based on the silhouette corresponding to the optimal value of k clusters.\n","Then, the program train the model on k clusters and predict labels and a title to each label.\n","The title is given by the nearest points of each centroids based on a metric (euclidean distance). We take the idf or bow of this articles and return the words which is as representative as possible of the cluster."]},{"cell_type":"markdown","metadata":{"id":"xU-jkPeZz_FE"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"id":"Y0uvidupvVwW"},"source":["# %matplotlib inline\n","import nltk\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm_notebook\n","from google.colab import drive\n","from nltk.corpus import stopwords\n","from sklearn.cluster import KMeans\n","from scipy.spatial import distance\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.metrics import silhouette_score\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# Execute this block of code only if you wish to avoid warnings in the output\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0xVyKPtS0o6c"},"source":["# Create link between drive and notebook"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"heuHmvdOvXMX","executionInfo":{"elapsed":925,"status":"ok","timestamp":1610742470330,"user":{"displayName":"Damien i","photoUrl":"","userId":"18198861212693999515"},"user_tz":-60},"outputId":"962eca1f-dee5-4a2f-f5c4-7b58448965a8"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i9_ZsSoszzwI"},"source":["# Importation of the data sets"]},{"cell_type":"code","metadata":{"id":"HYbHCsWmyCGs"},"source":["data_innovation = pd.read_json(\n","    \"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Output/Innovation/Res_Innovation_semi_supervised_bow.json\")\n","data_gestion = pd.read_json(\n","    \"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Output/Innovation/Res_gestion_semi_supervised_V1.json\")\n","data = pd.read_json(\n","    \"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Input/g3_BOW_v1.json\")\n","data_2 = data_innovation.merge(\n","    data_gestion, on=\"art_id\").merge(data, on=\"art_id\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ipqoM_hIz0tL"},"source":["# Functions\n","\n","## First Part:\n","We find the number optimal of cluster, we train and we predict."]},{"cell_type":"code","metadata":{"id":"mGniftqnz1Ll"},"source":["# Find the optimal number of clusters, k\n","def optimal_cluster(X: np.ndarray) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        X: Dataset containint all our datas we want to train the model with\n","\n","    Out:\n","        Optimal value of clusters, k\n","    \"\"\"\n","    list_error_silhouette: list = []\n","    list_k: list = []\n","    K: range = range(9, 26)\n","\n","    # Create the range of k which are going to be used in our loop\n","    for k in tqdm_notebook(K):\n","        km: KMeans = KMeans(n_clusters=k, random_state=42)\n","        y = km.fit_predict(X)\n","        list_k.append(k)\n","        # Create a list with all k values\n","        list_error_silhouette.append(silhouette_score(X, y))\n","\n","    # Return the maximum of the error based on the silhouette corresponding to the optimal value of k\n","    return (list_k[np.argmax(list_error_silhouette)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIBzULfnMBF8"},"source":["# Train the chosen model on X matrix with the chosen number of clusters (nb_cluster) and return the trained model for later use\n","def training(X: np.ndarray, nb_cluster: int) -> np.ndarray:\n","    \"\"\"Documentation\n","    Parameters:\n","        X: Dataset containint all our datas we want to train the model with\n","        nb_cluster: Number cluster\n","\n","    Out:\n","        km: The model of KMedoids\n","    \"\"\"\n","    # kmeans\n","    km: KMeans = KMeans(n_clusters=nb_cluster, random_state=42, n_init=10)\n","\n","    # Fit the kmeans model on the data\n","    km.fit(X)\n","\n","    return km"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxGVmd0hMB7B"},"source":["# Predict with the kmeans model labels on the data\n","def predict(X: pd.DataFrame, model) -> np.ndarray:\n","    \"\"\"Documentation\n","    Parameters:\n","        X: Dataset containint all our datas we want to train the model with\n","        model: Unsupervied model\n","\n","    Out:\n","        Model prediction\n","    \"\"\"\n","    \n","    # Prediction\n","    return model.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XAsmjYTQ08tn"},"source":["## Second Part :\n","We find the title of all the clusters"]},{"cell_type":"code","metadata":{"id":"Nh_0GpCpNMEm"},"source":["# Calculate the chosen distance (euclidean or cosine) between the values inside\n","def compute_distance(v: np.ndarray, km: KMedoids, label: int) -> list:\n","    \"\"\"Documentation\n","    Parameters:\n","        v: The article to compute\n","        km: The model of KMedoids\n","        label: The number of the cluster\n","\n","    Out:\n","        Distance cosine \n","    \"\"\"\n","    # The cluster with label denomination and the centroid of this cluster\n","    return distance.cosine(v, km.cluster_centers_[label])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNVgnUmhz6PH"},"source":["# Word creation thanks to the features of the articles\n","def obtain_word(list_content: list, type: str = \"idf\") -> str:\n","    \"\"\"Documentation\n","    Parameters:\n","        list_content: A list of articles\n","        type: Type of word embedding (so \"idf\" for TF-IDF and \"bow\" for Bag-of-Word)\n","\n","    Out:\n","        String word\n","    \"\"\"\n","    # Vectorizes all the tags in the train set while cleaning it from stop words\n","    if type == \"idf\":\n","        vectorizer = TfidfVectorizer(stop_words=\"english\")\n","        X = vectorizer.fit_transform(list_content)\n","\n","    # Vectorizes all the tags in the train set while cleaning it from stop words\n","    elif type == \"bow\":\n","        vectorizer = CountVectorizer(stop_words=\"english\")\n","        X = vectorizer.fit_transform(list_content)\n","\n","    # Return an error while the type specified is not idf or bow\n","    else:\n","        print(\"None\")\n","\n","    # Create a word column from feature integer indices to feature name\n","    temp_data = pd.DataFrame(np.array(np.sum(X, axis=0))[0], columns=[\"score\"])\n","    temp_data[\"word\"] = vectorizer.get_feature_names()\n","    temp_data = temp_data.sort_values(by=[\"score\"], ascending=False)\n","\n","    return temp_data.iloc[0, 1] + \"/\" + temp_data.iloc[1, 1] + \"/\" + temp_data.iloc[2, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_qIhrGnNSib"},"source":["# Get the contnt of an article except the words present in the whitelist\n","def whitelist_content(list_content: list, list_whitelist: list) -> np.ndarray:\n","    \"\"\"Documentation\n","    Parameters:\n","        list_content: A list of articles\n","        list_whitelist: A whitelist of word\n","        \n","    Out:\n","        final_content: A list of articles without the word in the whitelist\n","    \"\"\"\n","    # We create a list\n","    final_content = []\n","\n","    # We take every word witch are not present in whitelist\n","    for content in list_content:\n","        temp_content = [w for w in content.split() if not w in list_whitelist]\n","        temp_content = \" \".join(temp_content)\n","        final_content.append(temp_content)\n","\n","    return np.array(final_content)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEvsv_30NRJ7"},"source":["# Allows you to obtain the labels for the articles\n","def get_label_title(data: pd.DataFrame, whitelist: list) -> dict:\n","    \"\"\"Documentation\n","    Parameters:\n","        data: A dataframe containing all our datas we want to work with\n","        whitelist: A whitelist of words we want to avoid in labels title\n","\n","    Out:\n","        themes: A string containing our title of the chosen label\n","    \"\"\"\n","    themes = {}\n","    # Loop on each label indices and get the title of the chosen label\n","    for label in np.unique(data[\"prediction\"]):\n","        sub_data = data.query(\"prediction == @label\").drop(\n","            [\"prediction\"], axis=1).copy()\n","        sub_data[\"score\"] = sub_data.drop([\"art_id\", \"art_content_clean_without_lem\"], axis=1).apply(\n","            compute_distance, axis=1, raw=True, args=(km, 0))\n","        head_content = sub_data.sort_values(by=[\"score\"], ascending=True).head(5)[\n","            \"art_content_clean_without_lem\"].values\n","        head_content = whitelist_content(head_content, whitelist)\n","        themes[label] = obtain_word(head_content, type=\"bow\")\n","    return themes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QISDYu629hAp"},"source":["## Third part :\n","We join the part one and the part two in one function"]},{"cell_type":"code","metadata":{"id":"zVXp9BeFz6Uu"},"source":["# Clusters the dataset with the optimal number of clusters and return\n","def predict_cluster(data: pd.DataFrame) -> pd.DataFrame::\n","    \"\"\"Documentation\n","    Parameters:\n","        data: A dataframe containing all our datas we want to work with\n","        \n","    Out:\n","        data[[\"art_id\",\"prediction\"]]: A dataframe containing our id and the title of the chosen label\n","        model: The model trained with the optimal number of clusters\n","    \"\"\"\n","    # We use the function higher to obtain the good number of clusters\n","    nb_cluster = optimal_cluster(\n","        data.drop([\"art_id\", \"art_content_clean_without_lem\"], axis=1))\n","    \n","    # nb_cluster=9\n","    print(\"We take \" + str(nb_cluster) + \" clusters.\")\n","    \n","    # We train the kmenoide, pca and normalisation to make prediction\n","    model = training(\n","        data.drop([\"art_id\", \"art_content_clean_without_lem\"], axis=1), nb_cluster)\n","    \n","    # We predict the cluster with the kmenoide, pca and normalisation trained\n","    data[\"prediction\"] = predict(\n","        data.drop([\"art_id\", \"art_content_clean_without_lem\"], axis=1), model)\n","    \n","    # We create a whitelist to drop some recurents word that was not drop\n","    whitelist = [\"plus\", \"tout\", \"cette\", \"ca\", \"etre\",\n","                 \"dire\", \"faut\", \"fait\", \"faire\", \"donc\"]\n","    \n","    # We create the label of each cluster\n","    data[\"prediction\"] = data[\"prediction\"].map(\n","        get_label_title(data, whitelist))\n","    \n","    return data[[\"art_id\", \"prediction\"]], model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gelQJc0IEgXQ"},"source":["## Fourth part :\n","Execution of all the functions"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","referenced_widgets":["e223ca94b3674ff0ac81d61f90567905","54c0eb8b2ef944cdab33a8fc7af19da7","720b7c62f279409f810526ed465c6da3","c9d3834a7e1645fdac1a0d382b279f5f","da9bfbaf2f824d1c8dd21e032a948cdb","b39ecc077ee545f6a297094a23d81b4d","64695c6bad3e453e983a0ef0bad3d106","7e654e406f854ea7b33d25fcef820b79"]},"id":"Zftqj3gwEXie","outputId":"8ae57584-e35b-4c6b-9d1f-d287f3277a2a"},"source":["# We take out the model because we need to save it to predict later\n","output, model = predict_cluster(data)\n","output.head(5)\n","\n","# For whitelist terms in the result\n","whitelist = [\"plus\", \"tout\", \"cette\", \"ca\", \"etre\", \"dire\", \"faut\", \"fait\", \"faire\", \"donc\", \"art_id\", \"entreprise\", \"gestion\", \"entreprises\",\n","             \"aussi\", \"meme\", \"bien\", \"ete\", \"comme\", \"etat\", \"ministre\", \"tres\", \"encore\", \"peut\", \"dont\", \"egalement\", \"notamment\", \"ainsi\", \"leurs\", \"entre\"]\n","\n","get_label_title(data, whitelist)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e223ca94b3674ff0ac81d61f90567905","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=17.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","We take 9 clusters.\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-7c43995a0b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#We take out the model because we need to save it to predict later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Pour whitelist des termes dans le résultat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-9b5c1d2f1e1d>\u001b[0m in \u001b[0;36mpredict_cluster\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"art_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"art_content_clean_without_lem\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mwhitelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"plus\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tout\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cette\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ca\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"etre\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dire\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"faut\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fait\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"faire\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"donc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_label_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhitelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"art_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-29d02171212f>\u001b[0m in \u001b[0;36mget_label_title\u001b[0;34m(data, whitelist)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#loop on each label indices and get the title of the chosen label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0msub_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction == @label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0msub_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"art_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"art_content_clean_without_lem\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_distance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mhead_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msub_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"art_content_clean_without_lem\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mhead_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitelist_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhitelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'km' is not defined"]}]}]}