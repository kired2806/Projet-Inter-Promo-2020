{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Innovation_V1_1_clean.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H9qXLCo124-Z"},"source":["Created on Friday 07 January 2021\n","\n","Group 5 - Classification\n","Extraction features syntaxe\n","\n","@authors : L.D. and F.B"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jYo9xd8g8YdK","executionInfo":{"status":"ok","timestamp":1610137297022,"user_tz":-60,"elapsed":3770,"user":{"displayName":"Lilian Dulinge","photoUrl":"","userId":"13554953740448951668"}},"outputId":"1f2e9f1e-136b-4590-b17f-69c4a4cc640e"},"source":["import pandas as pd \n","import numpy as np\n","import re\n","from tqdm import tqdm_notebook as tqdm\n","from sklearn.model_selection import train_test_split\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import svm\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","\n","from sklearn.metrics import f1_score\n","from string import punctuation\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XrufXQJbG2W6","executionInfo":{"status":"ok","timestamp":1610137335521,"user_tz":-60,"elapsed":33252,"user":{"displayName":"Lilian Dulinge","photoUrl":"","userId":"13554953740448951668"}},"outputId":"5e659cfa-d406-4910-dfaf-acb4ec8f6a87"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sezPq29iKKv4"},"source":["df_analyse = pd.read_json(\"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Input/df_final_clean.json\")\n","df_analyse.drop('art_content', axis=1, inplace=True)\n","\n","df_train = pd.read_csv(\"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Input/Data_With_Features_Syntax.csv\")\n","\n","df_train = pd.merge(left=df_analyse, right=df_train, left_on='art_id', right_on='art_id')\n","\n","df_lexique: pd.DataFrame = pd.read_csv(\"/content/drive/MyDrive/G5 Inter-Promo 2021/Ressources/Lexique_Innovation.txt\", sep=\"  \", header=None)\n","df_lexique.columns: list = ['key_word']\n","\n","list_key_word = df_lexique.key_word.values.tolist()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJLKrJ1SNla2"},"source":["# Counts the number of words\n","def nb_word(text: str) -> int:\n","    \"\"\"Documentation\n","      Parameters:\n","        text: text of the article\n","\n","      Out (if exists):\n","        nb_word: number of word in  the text\n","    \"\"\"\n","    nb_words: list = []\n","    nb: int = 0\n","    # removes special characters\n","    for p in punctuation:\n","      text= text.replace(p, ' ')\n","\n","    # counts the number of words present in the text\n","    return len(text.split())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6nQr7i-Sc9Di"},"source":["'''\n","def clean_keyword(list_key_word : list):\n","  list_key_word_net : list = []\n","  for k in list_key_word :\n","    k=k.lower()\n","    k=k.replace(' de ',' ')\n","    k=k.replace(\" d' \",' ')\n","    k=k.replace(\".\",\" \")\n","    k=k.replace(\"-\",\" \")\n","    k=k.replace(\" du \",\" \")\n","    list_key_word_net.append(k)\n","  list_key_word_net = pd.unique(list_key_word_net).tolist()\n","  return list_key_word\n","\n","#list_key_word = clean_keyword(list_key_word)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kmj9NsARN7dc","colab":{"base_uri":"https://localhost:8080/","height":132},"executionInfo":{"status":"error","timestamp":1610137406478,"user_tz":-60,"elapsed":1144,"user":{"displayName":"Lilian Dulinge","photoUrl":"","userId":"13554953740448951668"}},"outputId":"473c9123-e3f8-471d-86a3-331dae57b4de"},"source":["# Count the number of time where the words in the list appear\n","\n","def count_key_words(text : str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: text of the article\n","\n","    Out (if exists):\n","        t : number of key word in  the text\n","    \"\"\"\n","    t : int = 0\n","\n","    if text != None:\n","        text = text.lower()\n","        text = text.split()\n","        for j in range(len(text)):\n","            if (text[j] in list_key_word):   #list_key_word: List of word that we will check in the sentences\n","                t += 1\n","            else : # For 2-word keywords\n","              try : # In case j+1 does not exist\n","                if text[j]+' '+text[j+1] in list_key_word :\n","                  t += 1\n","                else : #For 3-word keywords\n","                  try : # In case j+2 does not exist\n","                    if text[j]+' '+text[j+1]+' '+text[j+2] in list_key_word :\n","                      t+=1\n","                  except :\n","                    pass\n","              except :\n","                pass\n","    return t"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-3e35035af32e>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    except\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"v-KNs8yIPUsY"},"source":["# Count the number of sentence\n","def phrases( text : str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: text of the article\n","\n","    Out (if exists):\n","        n:  The number of sentence in a article\n","    \"\"\"\n","\n","    n : int = 0\n","   \n","    if not isinstance(text, str):\n","        text: str = str(text)\n","\n","    if (text != None):\n","        text = text.replace(\"..\", \".\")\n","        text = text.replace(\"...\", \".\")\n","        text = text.replace(\"!\", \".\")\n","        text = text.replace(\"!!\", \".\")\n","        text = text.replace(\"!!!\", \".\")\n","        text = text.replace(\"?\", \".\")\n","        text = text.replace(\"??\", \".\")\n","        text = text.replace(\"???\", \".\")\n","        text = text.replace(\"?!\", \".\")\n","        text = text.replace(\"!?\", \".\")\n","        n = len(sent_tokenize(text))\n","\n","    return n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVG7Qr7VIEE1"},"source":["def innovation(data_nb1, data_nb2, data_ratio1, data_ratio2, data_ratio3):\n","  res = []\n","  seuil = 0.05\n","  for i in tqdm(range(len(data_nb1))):\n","    valeur = data_nb1[i]*data_ratio1[i] + 0.5*(data_nb2[i]*data_ratio2[i]) + 0.1*data_ratio3[i]\n","    if valeur > seuil :\n","      res.append(1)\n","    elif ((valeur < seuil) & (data_nb1[i] == 0) & (data_nb2[i] == 0)):\n","      res.append(0)\n","    elif ((valeur < seuil) & (data_ratio1[i] < 0.0005) & (data_nb2[i] < 0.001)):\n","      res.append(0)\n","    else :\n","      res.append('?')\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vmbon3NcGu3y"},"source":["def score_innovation(data: pd.DataFrame, weight_content: float , weight_title:float)-> pd.DataFrame:\n","  ''' This function calculates the score for each article\n","    Input :\n","      weight_content : weight of the content\n","      weight_title : weight of the title\n","    Output:\n","      data : dataframe with the label_innovation score column \n","  '''\n","  # centred and reduced\n","  data_scale : pd.DataFrame= data[['Nb_key_words','Nb_key_words_title']]\n","  data_scale = scaler.fit_transform(data_scale)\n","\n","  # calculate the score \n","  data['label_innovation'] = data_scale[:,0]*weight_content + data_scale[:,1]*weight_title # innovation score \n","\n","  # Extend over the interval 0 and 1\n","  i=max(data['label_innovation'].values)\n","  j=min(data['label_innovation'].values)\n","  data['label_innovation']=(data['label_innovation'] - j)/(i - j)\n","\n","  return data\n","\n","\n","def classification_innovation( score_threshold : tuple) -> int:\n","  ''' This function classifies each article, 1 if it is innovative 0 otherwise\n","     Input :\n","        score_threshold : tuple containing the score and the threshold \n","  '''\n","  score, threshold = score_threshold[0], score_threshold[1]\n","  if score <= threshold:\n","    return 0\n","  else:\n","    return 1\n","\n","\n","def classification_innovation_2(data: pd.DataFrame, weight_content: float = 0.7, weight_title:float = 0.3, threshold:float = 0.6)-> pd.DataFrame :\n","  ''' This function calculates the innovation score and then classifies each article. \n","    Input:\n","      threshold : threshold , default = 0.6 : optimum thresholding\n","      weight_content : weight of the content, default = 0.7 : the optimum weight\n","      weight_title : weight of the title, default = 0.3 : the optimum weight\n","    Output:\n","      data : dataframe with the label_innovation score column \n","  '''\n","  data = score_innovation(data, weight_content, weight_title)\n","  data['label_innovation']=list(zip(data['label_innovation'].values, np.repeat(threshold,len(data))))\n","  data['label_innovation']=data['label_innovation'].apply(classification_innovation)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0kPaOasQI6S"},"source":["def create_features(data : pd.DataFrame, text : str, title : str):\n","  #\n","  data[\"Nb_key_words\"] : np.DataFrame = data[text].apply(count_key_words)\n","  data[\"Nb_key_words_title\"] : np.DataFrame = data[title].apply( count_key_words)\n","  data[\"Nb_words\"]: np.DataFrame = data[text].apply(nb_word)\n","  data[\"Nb_words_title\"]: np.DataFrame = data[title].apply(nb_word)\n","  data[\"Nb_sentences\"]: np.DataFrame = data['art_content'].apply(phrases)\n","  data[\"average_word_sentence\"]: np.DataFrame = data[\"Nb_words\"] / data[\"Nb_sentences\"]\n","  data['ratio_key_words']: np.DataFrame  = data['Nb_key_words']/data['Nb_words']\n","  data['ratio_key_words']: np.DataFrame  = data['ratio_key_words'].fillna(0)\n","  data['ratio_key_sentences']: np.DataFrame  = data['Nb_key_words']/data['Nb_sentences']\n","  data['ratio_key_sentences']: np.DataFrame  = data['ratio_key_sentences'].fillna(0)\n","  data['ratio_key_word_title']: np.DataFrame  = data['Nb_key_words_title'] / data['Nb_words_title']\n","  data['ratio_key_word_title']: np.DataFrame  = data['ratio_key_word_title'].fillna(0)\n","\n","  #\n","  data['innovation'] = innovation(data['Nb_key_words'],data['ratio_key_words'],data['Nb_key_words_title'],data['ratio_key_word_title'],data['ratio_key_sentences'])\n","  #\n","\n","  data = classification_innovation_2 (data = data)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"kca479EmIE08","executionInfo":{"status":"error","timestamp":1610137446622,"user_tz":-60,"elapsed":1113,"user":{"displayName":"Lilian Dulinge","photoUrl":"","userId":"13554953740448951668"}},"outputId":"5b42d1f3-3ecc-4039-ce47-bbefcea7e068"},"source":["%%time\n","df_train = create_features(df_train, 'art_content_prepd', 'art_title')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-39a525f0e5aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"df_train = create_features(df_train, 'art_content_prepd', 'art_title')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-10-c1287312ae94>\u001b[0m in \u001b[0;36mcreate_features\u001b[0;34m(data, text, title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Nb_key_words\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_key_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Nb_key_words_title\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcount_key_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Nb_words\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'count_key_words' is not defined"]}]},{"cell_type":"code","metadata":{"id":"krCFTHuMU-jV"},"source":["var_utile = df_train[['art_id','Nb_key_words','Nb_key_words_title','ratio_key_words','ratio_key_word_title']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qYalnKi1ILee"},"source":["def create_label(features : list, df_train : pd.DataFrame, data_type : str):\n","  df_train = df_train.sample(frac=1, random_state=15).reset_index(drop=True)\n","  df_train['index'] = df_train.index\n","  \n","  var_utile = df_train[features]\n","  var_utile = pd.concat([var_utile, df_train['innovation']], axis = 1)\n","  \n","  tout = var_utile[var_utile['innovation'] != '?']\n","  unlabeled = var_utile[var_utile['innovation'] == '?']\n","  \n","  X_train = tout.drop('innovation', axis=1)\n","  y_train = tout.innovation\n","  y_train = pd.to_numeric(y_train)\n","\n","  X_unlabeled = unlabeled.drop('innovation', axis=1)\n","\n","  if (data_type == 'key'):\n","    model1 = LogisticRegression(C=1e5)\n","    model2 = KNeighborsClassifier()\n","    model3 = make_pipeline(StandardScaler(),\n","                     SGDClassifier(max_iter=1000, tol=1e-3,loss = 'log'))\n","  elif (data_type == 'BOW'):\n","    model1 = svm.SVC(C = 4,kernel='linear', probability= True)\n","    model2 = LogisticRegression(C=1e5)\n","    model3 = make_pipeline(StandardScaler(),\n","                     SGDClassifier(max_iter=1000, tol=1e-3,loss = 'log'))\n","  # Initiate iteration counter\n","  iterations = 0\n","\n","  # Containers to hold f1_scores and # of pseudo-labels\n","  train_f1s = []\n","  pseudo_labels = []\n","\n","  # Assign value to initiate while loop\n","  high_prob = [1] \n","\n","  # Loop will run until there are no more high-probability pseudo-labels\n","  while len(high_prob) > 0:\n","        \n","    # Fit classifier and make train/test predictions\n","    model1.fit(X_train, y_train)\n","    y_hat_train = model1.predict(X_train)\n","\n","    # Calculate and print iteration # and f1 scores, and store f1 scores\n","    train_f1 = f1_score(y_train, y_hat_train)\n","    train_f1s.append(train_f1)\n","   \n","    if (len(X_unlabeled) > 0):\n","      # Generate predictions and probabilities for unlabeled data\n","\n","      pred_probs : np.ndarray = model1.predict_proba(X_unlabeled)\n","      preds : np.ndarray = model1.predict(X_unlabeled)\n","      prob_0 : list = pred_probs[:,0]\n","      prob_1 : list = pred_probs[:,1]\n","\n","      # Store predictions and probabilities in dataframe\n","      df_pred_prob : pd.DataFrame = pd.DataFrame([])\n","      df_pred_prob['preds'] = preds\n","      df_pred_prob['prob_0'] = prob_0\n","      df_pred_prob['prob_1'] = prob_1\n","      df_pred_prob.index = X_unlabeled.index\n","    \n","      # Separate predictions with > 99% probability\n","      high_prob : pd.DataFrame = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.99],\n","                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.99]],\n","                          axis=0)\n","    \n","      pseudo_labels.append(len(high_prob))\n","\n","      # Add pseudo-labeled data to training data\n","      X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n","      y_train = pd.concat([y_train, high_prob.preds])      \n","    \n","      # Drop pseudo-labeled instances from unlabeled data\n","      X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n","    \n","      # Update iteration counter\n","      iterations += 1\n","    else :\n","      high_prob = []\n","\n","  # Initiate iteration counter\n","  iterations = 0\n","\n","  # Containers to hold f1_scores and # of pseudo-labels\n","  train_f1s = []\n","  pseudo_labels = []\n","\n","  # Assign value to initiate while loop\n","  high_prob = [1] \n","\n","  # Loop will run until there are no more high-probability pseudo-labels\n","  while len(high_prob) > 0:\n","        \n","    # Fit classifier and make train/test predictions\n","    model2.fit(X_train, y_train)\n","    y_hat_train = model2.predict(X_train)\n","\n","    # Calculate and print iteration # and f1 scores, and store f1 scores\n","    train_f1 = f1_score(y_train, y_hat_train)\n","    train_f1s.append(train_f1)\n","   \n","    if (len(X_unlabeled) > 0):\n","      # Generate predictions and probabilities for unlabeled data\n","\n","      pred_probs : np.ndarray = model2.predict_proba(X_unlabeled)\n","      preds : np.ndarray = model2.predict(X_unlabeled)\n","      prob_0 : list = pred_probs[:,0]\n","      prob_1 : list = pred_probs[:,1]\n","\n","      # Store predictions and probabilities in dataframe\n","      df_pred_prob : pd.DataFrame = pd.DataFrame([])\n","      df_pred_prob['preds'] = preds\n","      df_pred_prob['prob_0'] = prob_0\n","      df_pred_prob['prob_1'] = prob_1\n","      df_pred_prob.index = X_unlabeled.index\n","    \n","      # Separate predictions with > 99% probability\n","      high_prob : pd.DataFrame = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.99],\n","                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.99]],\n","                          axis=0)\n","    \n","      pseudo_labels.append(len(high_prob))\n","\n","      # Add pseudo-labeled data to training data\n","      X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n","      y_train = pd.concat([y_train, high_prob.preds])      \n","    \n","      # Drop pseudo-labeled instances from unlabeled data\n","      X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n","    \n","      # Update iteration counter\n","      iterations += 1\n","    else :\n","      high_prob = []\n","\n","  # Initiate iteration counter\n","  iterations = 0\n","\n","  # Containers to hold f1_scores and # of pseudo-labels\n","  train_f1s = []\n","  pseudo_labels = []\n","\n","  # Assign value to initiate while loop\n","  high_prob = [1] \n","\n","  # Loop will run until there are no more high-probability pseudo-labels\n","  while len(high_prob) > 0:\n","        \n","    # Fit classifier and make train/test predictions\n","    model3.fit(X_train, y_train)\n","    y_hat_train = model3.predict(X_train)\n","\n","    # Calculate and print iteration # and f1 scores, and store f1 scores\n","    train_f1 = f1_score(y_train, y_hat_train)\n","    train_f1s.append(train_f1)\n","   \n","    if (len(X_unlabeled) > 0):\n","      # Generate predictions and probabilities for unlabeled data\n","\n","      pred_probs : np.ndarray = model3.predict_proba(X_unlabeled)\n","      preds : np.ndarray = model3.predict(X_unlabeled)\n","      prob_0 : list = pred_probs[:,0]\n","      prob_1 : list = pred_probs[:,1]\n","\n","      # Store predictions and probabilities in dataframe\n","      df_pred_prob : pd.DataFrame = pd.DataFrame([])\n","      df_pred_prob['preds'] = preds\n","      df_pred_prob['prob_0'] = prob_0\n","      df_pred_prob['prob_1'] = prob_1\n","      df_pred_prob.index = X_unlabeled.index\n","    \n","      # Separate predictions with > 60% probability\n","      high_prob : pd.DataFrame = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.60],\n","                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.60]],\n","                          axis=0)\n","    \n","      pseudo_labels.append(len(high_prob))\n","\n","      # Add pseudo-labeled data to training data\n","      X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n","      y_train = pd.concat([y_train, high_prob.preds])      \n","    \n","      # Drop pseudo-labeled instances from unlabeled data\n","      X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n","    \n","      # Update iteration counter\n","      iterations += 1\n","    else :\n","      high_prob = []\n","\n","  X_train['innovation'] = y_train\n","  X_train['index'] = X_train.index\n","  df_final = pd.merge(left=df_train, right=X_train, left_on='index', right_on='index')\n","  df_final = df_final[['art_id','innovation_y']]\n","  df_final = df_final.rename(columns = {'innovation_y': 'innovation'})\n","  return df_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jn5Oer6trZO_","colab":{"base_uri":"https://localhost:8080/","height":573},"executionInfo":{"status":"error","timestamp":1610138576800,"user_tz":-60,"elapsed":1089,"user":{"displayName":"Lilian Dulinge","photoUrl":"","userId":"13554953740448951668"}},"outputId":"142761d2-c1e8-4cdb-d0e2-f992a99a9791"},"source":["df_train_supervised = create_label(['Nb_key_words','Nb_key_words_title','ratio_key_words','ratio_key_word_title'], df_train, 'key')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'innovation'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-203f3db54b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train_supervised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Nb_key_words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Nb_key_words_title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ratio_key_words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ratio_key_word_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-9abd3881b570>\u001b[0m in \u001b[0;36mcreate_label\u001b[0;34m(features, df_train, data_type)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mvar_utile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mvar_utile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_utile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'innovation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_utile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_utile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'innovation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'innovation'"]}]},{"cell_type":"code","metadata":{"id":"5xlsVMlKaC8f"},"source":["df_train.drop('innovation', axis=1, inplace=True)\n","df_train = df_train.merge(df_train_supervised)\n","\n","df_train.to_csv('/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Output/Data_With_Features_Syntax_and_Label.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOQ_lZwycgUk"},"source":[""],"execution_count":null,"outputs":[]}]}