{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"g5_extraction_features_syntaxe_v0_1.ipynb","provenance":[{"file_id":"1NC_mADtkAAE4OOlMB9OaZskjBiBckDDT","timestamp":1609855465277}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"XnkCjEaJJxxd"},"source":["Created on Thursday 07 January 2021\n","\n","**Group 5 - Classification**  \n","**Extraction features syntaxe**\n","\n","@authors : J.J. and F.B"]},{"cell_type":"markdown","metadata":{"id":"0bdzHB4aYdJW"},"source":["This Notebook allows to extract syntax features on scrapped articles. "]},{"cell_type":"markdown","metadata":{"id":"UOszzz9TEMhd"},"source":["# Import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cF_IrxS4ELiC","executionInfo":{"status":"ok","timestamp":1610093776190,"user_tz":-60,"elapsed":3252,"user":{"displayName":"Fallou BAH","photoUrl":"","userId":"02816076045651275158"}},"outputId":"5bb67c3c-38e7-45a1-babe-80a0f5465c44"},"source":["import re\n","import os\n","import nltk\n","import tqdm\n","import string\n","import warnings\n","import numpy as np\n","import pandas as pd\n","\n","from string import punctuation\n","from textblob import TextBlob\n","from urllib.parse import urlparse\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","nltk.download('punkt')\n","nltk.download('vader_lexicon')\n","warnings.filterwarnings('ignore')\n","analyzer = SentimentIntensityAnalyzer()\n","\n","os.chdir(\"/content/drive/My Drive/G5 Inter-Promo 2021/Données/Input\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JHAWCjprEPzL"},"source":["# Creation of the link between the drive and the notebook"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7uheIDyDOP5","executionInfo":{"status":"ok","timestamp":1610093738892,"user_tz":-60,"elapsed":20007,"user":{"displayName":"Fallou BAH","photoUrl":"","userId":"02816076045651275158"}},"outputId":"f3c06c98-2d01-42d8-bc85-e2fd7ccf9e7d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KHnXNqpAEXHk"},"source":["# Import data"]},{"cell_type":"code","metadata":{"id":"5oIYK6UJEWVl"},"source":["data: pd.DataFrame = pd.read_json(\"Data.json\")\n","data.fillna(' ',inplace=True )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xdIA8gjH1Ss"},"source":["df_lexique: pd.DataFrame = pd.read_csv(\"/content/drive/My Drive/G5 Inter-Promo 2021/Ressources/Lexique_Innovation.txt\", sep=\"  \", header=None)\n","df_lexique.columns: list = ['mots_cle']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PWhe29BREc92"},"source":["# Features extraction"]},{"cell_type":"code","metadata":{"id":"k6w--UCBEb10"},"source":["# Counts the number of words\n","def nb_word(text: str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: Text of the article\n","\n","    Out (if exists):\n","        nb_word: Number of word in  the text\n","    \"\"\"\n","    nb_words: list = []\n","    nb: int = 0\n","\n","    # Removes special characters\n","    for p in punctuation:\n","      text= text.replace(p, ' ')\n","\n","    # Counts the number of words present in the text\n","    return len(text.split())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGUI7kyzG61J"},"source":["# Count the number of time where the words in the list appear\n","def count_key_words(text : str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: Text of the article\n","\n","    Out:\n","        t: Number of key word in  the text\n","    \"\"\"\n","    t : int = 0\n","\n","    if text != None:\n","        text = text.lower()\n","        text = text.split()\n","        for j in text:\n","            if (j in list_mot_cle):   #list_mot_cle: List of word that we will check in the sentences\n","                t = t + 1\n","    return t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjqNP__UG6zA"},"source":["# Count the number of time where a word appear\n","def Word_Apparition(tup: tuple ) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","      tup: Tuple containing the of the article and the word that we will check\n","\n","    Out:\n","        iter: The number of time where the word appear in a article\n","    \"\"\"\n","    try :\n","      comm, word = tup[0], tup[1]\n","    except :\n","      return 0\n","\n","    iter: int = 0\n","\n","    if not isinstance(comm, str):\n","        comm: str = str(comm)\n","\n","    if comm != None :\n","        for j in range(len(comm)):\n","            if comm[j] == word:\n","                iter += 1\n","    return iter "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sF353WErHnWh"},"source":["# Count the number of sentence\n","def phrases( text : str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: Text of the article\n","\n","    Out:\n","        n:  The number of sentence in a article\n","    \"\"\"\n","    n : int = 0\n","   \n","    if not isinstance(text, str):\n","        text: str = str(text)\n","\n","    if (text != None):\n","        text = text.replace(\"..\", \".\")\n","        text = text.replace(\"...\", \".\")\n","        text = text.replace(\"!\", \".\")\n","        text = text.replace(\"!!\", \".\")\n","        text = text.replace(\"!!!\", \".\")\n","        text = text.replace(\"?\", \".\")\n","        text = text.replace(\"??\", \".\")\n","        text = text.replace(\"???\", \".\")\n","        text = text.replace(\"?!\", \".\")\n","        text = text.replace(\"!?\", \".\")\n","        n = len(sent_tokenize(text))\n","\n","    return n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJa66kDjG6xB"},"source":["def sentiment_analisys_positive(text: str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: An article\n","    Out (if exists):\n","        The positive score corresponding to the article\n","    \"\"\"\n","    scores = analyzer.polarity_scores(text)\n","    return(scores['pos'])\n","\n","\n","def sentiment_analisys_negative(text: str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: An article\n","    Out (if exists):\n","        The negative score corresponding to the article\n","    \"\"\"\n","    scores = analyzer.polarity_scores(text)\n","    return(scores['neg'])\n","\n","\n","def get_polarity(text: str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: An article\n","    Out (if exists):\n","        The polarity score corresponding to the article\n","    \"\"\"\n","    return(TextBlob(text).sentiment.polarity)\n","\n","\n","def get_sentiment_sujectivity(text: str) -> int:\n","    \"\"\"Documentation\n","    Parameters:\n","        text: An article\n","    Out (if exists):\n","        The subjectivity score corresponding to the article\n","    \"\"\"\n","    return(TextBlob(text).sentiment.subjectivity)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZALlYCCHtAu"},"source":["list_mot_cle :list = df_lexique[\"mots_cle\"].values.tolist()\n","\n","def compute_features(data: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Documentation\n","    Parameters:\n","        data: A dataframe with the initial content\n","    Out (if exists):\n","        data: A dataframe with new features based on the articles\n","    \"\"\"\n","\n","    # Compute features on the content\n","    data[\"Nb_key_words\"] : np.DataFrame = data[\"art_content\"].apply(count_key_words)\n","    data[\"Nb_key_words_title\"] : np.DataFrame = data[\"art_title\"].apply( count_key_words)\n","    data[\"Nb_words\"]: np.DataFrame = data[\"art_content\"].apply(nb_word)\n","    data[\"Nb_words_title\"]: np.DataFrame = data[\"art_title\"].apply(nb_word)\n","    data[\"Nb_sentences\"]: np.DataFrame = data[\"art_content\"].apply(phrases)\n","\n","    data[\"average_word_sentence\"]: np.DataFrame = data[\"Nb_words\"] / data[\"Nb_sentences\"]\n","    data[\"ratio_word_title_on_word\"]: np.DataFrame  = data[\"Nb_words_title\"] / data[\"Nb_words\"]\n","\n","    data['exclamation']=list(zip(data['art_content'].values, np.repeat('!',len(data))))\n","    data[\"exclamation\"]: np.DataFrame  = data[\"art_content\"].apply(Word_Apparition)\n","\n","    data['interrogation']=list(zip(data['art_content'].values, np.repeat('?',len(data))))\n","    data[\"interrogation\"]: np.DataFrame  = data[\"art_content\"].apply(Word_Apparition)\n","\n","\n","    data['ratio_key_words']: np.DataFrame  = data['Nb_key_words']/data['Nb_words']\n","    data['ratio_key_words']: np.DataFrame  = data['ratio_key_words'].fillna(0)\n","    data['ratio_key_sentences']: np.DataFrame  = data['Nb_key_words']/data['Nb_sentences']\n","    data['ratio_key_sentences']: np.DataFrame  = data['ratio_key_sentences'].fillna(0)\n","    data['ratio_key_word_title']: np.DataFrame  = data['Nb_key_words_title'] / data['Nb_words_title']\n","    data['ratio_key_word_title']: np.DataFrame  = data['ratio_key_word_title'].fillna(0)\n","\n","    # Compute features on URL\n","    data[\"netloc\"]: np.DataFrame  = data[\"art_url\"].apply(lambda x: urlparse(x).netloc)\n","    data[\"netloc.com\"]: np.DataFrame  = data[\"netloc\"].apply(\n","        lambda x: re.findall(\"\\.[a-z]+\", x))\n","    data[\"nb_netloc.com\"]: np.DataFrame  = data[\"netloc.com\"].apply(lambda x: len(x))\n","    data[\"path\"]: np.DataFrame  = data[\"art_url\"].apply(lambda x: urlparse(x).path)\n","    data[\"nb_word_path\"]: np.DataFrame  = data[\"path\"].apply(lambda x: len(\n","        x.replace(\"/\", \" \").replace(\"-\", \" \").replace(\"_\", \" \").split()))\n","\n","    data.drop([\"path\", \"netloc\", \"nb_netloc.com\"], axis=1, inplace=True)\n","\n","    # Compute features on sentiment analisys\n","    data[\"content_postive_score\"]: np.DataFrame  = data['art_content'].apply(\n","        sentiment_analisys_positive)\n","    data[\"title_postive_score\"]: np.DataFrame  = data['art_title'].apply(\n","        sentiment_analisys_positive)\n","\n","    data[\"content_negative_score\"]: np.DataFrame  = data['art_content'].apply(\n","        sentiment_analisys_negative)\n","    data[\"title_negative_score\"]: np.DataFrame  = data['art_title'].apply(\n","        sentiment_analisys_negative)\n","\n","    data[\"content_polarity_score\"]: np.DataFrame  = data['art_content'].apply(get_polarity)\n","    data[\"title_polarity_score\"]: np.DataFrame  = data['art_title'].apply(get_polarity)\n","\n","    data[\"content_subjectivity_score\"]: np.DataFrame  = data['art_content'].apply(\n","        get_sentiment_sujectivity)\n","    data[\"title_subjectivity_score\"]: np.DataFrame  = data['art_title'].apply(\n","        get_sentiment_sujectivity)\n","\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YDXK7CzqQkKa"},"source":["# We use the function to compute the features"]},{"cell_type":"code","metadata":{"id":"XluSWGnGHs6-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610035260044,"user_tz":-60,"elapsed":558200,"user":{"displayName":"Fallou BAH","photoUrl":"","userId":"02816076045651275158"}},"outputId":"cc851f0f-cfab-490d-d07a-431cb41e95ac"},"source":["%%time\n","data: pd.DataFrame  = compute_features(data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 9min 11s, sys: 2.41 s, total: 9min 14s\n","Wall time: 9min 17s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tNsBVeS5GtRT"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTqEqSAmaD6U"},"source":["#data.query(\"art_lang == 'fr'\").to_csv(\n","#    \"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Input/Data_With_Features_Syntax.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eCP6ZPrmBsw6"},"source":[""],"execution_count":null,"outputs":[]}]}