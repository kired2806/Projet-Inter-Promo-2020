{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"g5_global_train_v0.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Z3WJ2JDgMPWU"},"source":["Created on Wednesday 13 January 2021\r\n","\r\n","**Group 5 - Classification**  \r\n","**Extraction features syntaxe**\r\n","\r\n","@authors : Jeremy Johann"]},{"cell_type":"markdown","metadata":{"id":"yn2EV2S5MMM-"},"source":["The notebook that contain all the code. You just need to put in the fonction the art_id, the content with lemmatization, the title of the article and the Bag Of Word (BOW). This code train, save all the model and give the prediction."]},{"cell_type":"markdown","metadata":{"id":"Q7ZLapoyzIf3"},"source":["# Pip install"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"97RchJsEb4oc","executionInfo":{"status":"ok","timestamp":1610887365320,"user_tz":-60,"elapsed":24590,"user":{"displayName":"Johann Jérémy","photoUrl":"","userId":"04957728469021807651"}},"outputId":"6171ca99-d2ac-4c4a-d1ae-9e91ded13f16"},"source":["pip install scikit-learn-extra"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting scikit-learn-extra\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/891d2ee7bd18af8f2e1df5d63a52ee96edd0eacc21bb9627072b1c5f6a6c/scikit-learn-extra-0.1.0b2.tar.gz (615kB)\n","\r\u001b[K     |▌                               | 10kB 14.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 19.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 122kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 133kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 143kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 163kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 184kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 194kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 204kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 215kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 225kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 235kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 245kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 256kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 266kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 276kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 286kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 296kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 307kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 317kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 327kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 337kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 358kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 368kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 378kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 389kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 399kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 409kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 419kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 430kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 440kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 450kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 460kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 471kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 481kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 491kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 501kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 512kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 522kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 532kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 542kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 552kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 563kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 573kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 583kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 593kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 604kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 614kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 4.0MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->scikit-learn-extra) (1.0.0)\n","Building wheels for collected packages: scikit-learn-extra\n","  Building wheel for scikit-learn-extra (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-learn-extra: filename=scikit_learn_extra-0.1.0b2-cp36-cp36m-linux_x86_64.whl size=339584 sha256=f920b1685b0f32eee0c8c751a201a504fb0dbf32aee082dfa1fcac9f9f759183\n","  Stored in directory: /root/.cache/pip/wheels/04/01/0f/943bffb48bac048fa216b4325f1a6c939491ccb0ff500e08f4\n","Successfully built scikit-learn-extra\n","Installing collected packages: scikit-learn-extra\n","Successfully installed scikit-learn-extra-0.1.0b2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aFYVMwkxzMCu"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"id":"Oy7uHDX9T0fS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610887368157,"user_tz":-60,"elapsed":27419,"user":{"displayName":"Johann Jérémy","photoUrl":"","userId":"04957728469021807651"}},"outputId":"04d3eea2-2bfd-450d-835d-ede43e392727"},"source":["import re\r\n","import os\r\n","import nltk\r\n","import math\r\n","import string\r\n","import pickle\r\n","import warnings\r\n","import numpy as np\r\n","import pandas as pd\r\n","\r\n","from tqdm import tqdm_notebook\r\n","from tqdm import tqdm_notebook as tqdm\r\n","\r\n","from xgboost import XGBClassifier\r\n","\r\n","from scipy.spatial import distance\r\n","\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem import WordNetLemmatizer\r\n","from nltk.tokenize import sent_tokenize, word_tokenize\r\n","\r\n","from sklearn import svm\r\n","from sklearn.manifold import TSNE\r\n","from sklearn.cluster import KMeans\r\n","from sklearn.decomposition import PCA\r\n","from sklearn_extra.cluster import KMedoids\r\n","from sklearn.pipeline import make_pipeline\r\n","from sklearn.naive_bayes import CategoricalNB\r\n","from sklearn.tree import DecisionTreeClassifier\r\n","from sklearn.preprocessing import StandardScaler\r\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\r\n","from sklearn.model_selection import train_test_split, StratifiedKFold\r\n","from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor\r\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n","from sklearn.metrics import f1_score, silhouette_score, calinski_harabasz_score\r\n","from sklearn.ensemble import BaggingClassifier, IsolationForest, RandomForestClassifier, ExtraTreesClassifier\r\n","\r\n","warnings.filterwarnings('ignore')\r\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"sX1RHLNi0QuI"},"source":["# Create link between drive and notebook"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qiIIWDqN3Wik","executionInfo":{"status":"ok","timestamp":1610887402020,"user_tz":-60,"elapsed":60930,"user":{"displayName":"Johann Jérémy","photoUrl":"","userId":"04957728469021807651"}},"outputId":"b6d1d2c7-b8e2-45e0-a0ff-7feb8620ba14"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qa_Dlu4cQZQW"},"source":["# Detection documents innovants et gamme de gestion"]},{"cell_type":"code","metadata":{"id":"LzG2w1ASQY2Z"},"source":["# Counts the number of words\r\n","def nb_word(text: list) -> int:\r\n","    \"\"\"Documentation\r\n","    Parameters:\r\n","        text: texts of the article\r\n","\r\n","    Out (if exists):\r\n","        nb_word: number of word in  the document\r\n","    \"\"\"\r\n","    nb_words: list = []\r\n","    nb: int = 0\r\n","\r\n","    # Browse through the different texts\r\n","    for i in text:\r\n","        # Removes special characters\r\n","        i.replace(',', ' ')\r\n","        i.replace('.', ' ')\r\n","        i.replace('!', ' ')\r\n","        i.replace('?', ' ')\r\n","        i.replace('/', ' ')\r\n","        # Creates a list with all the words present in the text\r\n","        list_words: list = i.split()\r\n","        # Counts the number of words present in the text\r\n","        nb_words.append(len(list_words))\r\n","\r\n","    return nb_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfvuyZoGQY0d"},"source":["# This function calculate the numbers of key words given by a list in the documents\r\n","def count_key_words(data: list, l: list) -> list:\r\n","    \"\"\"Documentation\r\n","    Parameters:\r\n","        data: List of articles\r\n","        l: List of word that we will check in the sentences\r\n","\r\n","    Out:\r\n","        res: List where each value is the number of time where key word appear in the article\r\n","    \"\"\"\r\n","    data = data.tolist()\r\n","    list_mot_unique=[]\r\n","    list_mot_compose=[]\r\n","\r\n","    # Count the number of time where the words in the list appear\r\n","    for elem in l :\r\n","      cpt = 0\r\n","      for car in elem :\r\n","        if (car == \" \") :\r\n","          cpt+=1\r\n","      if cpt==0 :\r\n","        list_mot_unique.append(elem)\r\n","      else :\r\n","        list_mot_compose.append(elem)\r\n","    res: list = []\r\n","\r\n","    for i in range(len(data)):\r\n","      sentence: str = data[i]\r\n","      if sentence is None:\r\n","        res.append(0)\r\n","      else:\r\n","        sentence = sentence.lower()\r\n","        sentence = sentence.split()\r\n","        t: int = 0\r\n","        for j in sentence:\r\n","          if (j in list(list_mot_unique)):\r\n","            t = t + 1\r\n","        sentence: str = data[i]\r\n","        for elem in list_mot_compose:\r\n","          if elem in sentence:\r\n","            t = t + 1\r\n","      res.append(t)\r\n","\r\n","    return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSJFMLbFQYx4"},"source":["# This function calculate the numbers of sentences in each documents\r\n","def phrases(data: pd.DataFrame, col: str) -> list:\r\n","    \"\"\"Documentation\r\n","    Parameters:\r\n","        data: Dataframe with all the data\r\n","        columns: The columns of the dataframe that we will use\r\n","\r\n","    Out:\r\n","        l: List where each value is the number of sentence in a article\r\n","    \"\"\"\r\n","    l: list = []\r\n","\r\n","    # Count the number of sentence\r\n","    for i in range(len(data[col])):\r\n","        sentences: str = data[col].tolist()[i]\r\n","\r\n","        if not isinstance(sentences, str):\r\n","            sentences: str = str(sentences)\r\n","\r\n","        if (sentences is None):\r\n","            count_sentence.append(0)\r\n","        else:\r\n","            sentences = sentences.replace(\"..\", \".\")\r\n","            sentences = sentences.replace(\"...\", \".\")\r\n","            sentences = sentences.replace(\"!\", \".\")\r\n","            sentences = sentences.replace(\"!!\", \".\")\r\n","            sentences = sentences.replace(\"!!!\", \".\")\r\n","            sentences = sentences.replace(\"?\", \".\")\r\n","            sentences = sentences.replace(\"??\", \".\")\r\n","            sentences = sentences.replace(\"???\", \".\")\r\n","            sentences = sentences.replace(\"?!\", \".\")\r\n","            sentences = sentences.replace(\"!?\", \".\")\r\n","            l.append(len(sent_tokenize(sentences)))\r\n","\r\n","    return l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmykhpejg5LK"},"source":["# count the number of distinct words in each documents\r\n","def count_words_diff(df: pd.DataFrame, list_key : list) -> int :\r\n","  \"\"\"Documentation\r\n","  Parameter:\r\n","      texte: text of an article\r\n","      \r\n","  Out:\r\n","      len(dico): lenght of a dictionary\r\n","  \"\"\"\r\n","  result = []\r\n","  for j in tqdm(range(len(df))):\r\n","    liste = df[j].split()\r\n","    dico = {}\r\n","    fait = False\r\n","    for i in range(len(liste)) :\r\n","      try : \r\n","        if (liste[i]+' '+liste[i+1]+' '+liste[i+2]) in list_key :\r\n","          dic(liste[i]+' '+liste[i+1]+' '+liste[i+2],dico)\r\n","          fait = True\r\n","        else :\r\n","          try :\r\n","            if (liste[i]+' '+liste[i+1]) in list_key :\r\n","              dic(liste[i]+' '+liste[i+1],dico)\r\n","              fait = True\r\n","          except :\r\n","            pass\r\n","      except :\r\n","        pass\r\n","      if not fait :\r\n","        dic(liste[i],dico)\r\n","      fait = False\r\n","    result.append(len(dico))\r\n","\r\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEX1RxL7g3V0"},"source":["# Return a dico with all the distinct word and their numbers of appearances\r\n","def comparaison_words_diff(texte: str, list_key : list) -> int :\r\n","  \"\"\"Documentation\r\n","    Parameter:\r\n","        texte: text of an article\r\n","\r\n","    Out:\r\n","        len(dico): lenght of a dictionary\r\n","  \"\"\"\r\n","  liste = texte.split()\r\n","  dico = {}\r\n","  fait = False\r\n","\r\n","  for i in range(len(liste)) :\r\n","    try : \r\n","      if (liste[i]+' '+liste[i+1]+' '+liste[i+2]) in list_key :\r\n","        dic(liste[i]+' '+liste[i+1]+' '+liste[i+2],dico)\r\n","        fait = True\r\n","      else :\r\n","        try :\r\n","          if (liste[i]+' '+liste[i+1]) in list_key :\r\n","            dic(liste[i]+' '+liste[i+1],dico)\r\n","            fait = True\r\n","        except :\r\n","          pass\r\n","    except :\r\n","      pass\r\n","    if not fait :\r\n","      dic(liste[i],dico)\r\n","    fait = False\r\n","    \r\n","  return dico"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBOg_-Grg2Kg"},"source":["# Count the distinct word of a list\r\n","def count_key_words_diff(liste : list) -> int:\r\n","  \"\"\"Documentation\r\n","  Parameter:\r\n","      liste: list of key words\r\n","\r\n","  Out:\r\n","      len(dico): lenght of a dictionary\r\n","  \"\"\"\r\n","  dico = {}\r\n","  for i in range(len(liste)) :\r\n","    dic(liste[i],dico)\r\n","\r\n","  return (dico)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ae6kibIjgzyQ"},"source":["# Look up a word in a dictionary\r\n","def dic(term: str,dico: dict) :\r\n","  \"\"\"Documentation\r\n","  Parameter:\r\n","      term: one or set of words\r\n","      dico: dictionary\r\n","  \"\"\"\r\n","  if term in dico.keys() :\r\n","    dico[term] += 1\r\n","  elif term != '' :\r\n","    dico[term] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"24hiT4kEgzWu"},"source":["# count the number of distinct key words of a list present in each text\r\n","def key_word_in_doc(df : pd.DataFrame, list_key : list):\r\n","  \"\"\"Documentation\r\n","    Parameter:\r\n","        df: Column of dataframe who contains all text\r\n","        list_key: List of keys word\r\n","  \"\"\"\r\n","  key_word = []\r\n","  for i in tqdm(range(len(df))):\r\n","    sortie =  comparaison_words_diff(df[i], list_key)\r\n","    liste_cle = []\r\n","    for cle in sortie.keys():\r\n","      liste_cle.append(cle)\r\n","    tot =0\r\n","    for i in liste_cle:\r\n","      if i in (list_key.tolist()):\r\n","        tot = tot + 1\r\n","    key_word.append(tot)\r\n","\r\n","  return (key_word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNgssJW7QYvi"},"source":["# Use to generate weighted randomness to say if a document talk about management range or not\r\n","def score_to_threshold(x: int) -> int:\r\n","    \"\"\"Documentation\r\n","    Parameters:\r\n","        x: Innovation score associated with an article\r\n","\r\n","    Out:\r\n","        Threshold probability used to differentiate innovative from non-innovative documents\r\n","    \"\"\"\r\n","\r\n","    return np.arctan(x * 100) / np.pi * 2 * 0.2 + 0.8\r\n","\r\n","\r\n","# Function to tell if a document talk about innovation or not \r\n","def innovation(data_nb1: pd.DataFrame, data_nb2: pd.DataFrame, data_nb3: pd.DataFrame, data_ratio1: pd.DataFrame, data_ratio2: pd.DataFrame, data_ratio3 : pd.DataFrame) -> list:\r\n","  \"\"\"Documentation\r\n","    Parameters:\r\n","          data_nb1: First column of a dataframe who describe a number of key words presents in text\r\n","          data_nb2: Second column of a dataframe who describe a number of key words presents in title\r\n","          data_nb3: Third column who describe the distinct number of key word presents in text\r\n","          data_ratio1: Linked column of data_nb1 who represents a ratio of key word apparition in text\r\n","          compared to the total number of words in text\r\n","          data_ratio2: Linked column of data_nb2 who represents a ratio of key word apparition in title\r\n","          compared to the total number of words in title\r\n","          data_ratio3: Linked column of data_nb3 who represents a ratio of distinct key word apparition in text\r\n","          compared to the total number of distinct word in text\r\n","\r\n","     Out:\r\n","          list: Represents score of innovation, calculate in the function innovation\r\n","  \"\"\"\r\n","  res: list = []\r\n","  seuil: int = 0.18\r\n","\r\n","  for i in tqdm(range(len(data_nb1))):\r\n","    valeur = 0.5*(data_nb1[i]*data_ratio1[i]) + 0.5*(data_nb2[i]*data_ratio2[i]) + (data_nb3[i]*data_ratio3[i])\r\n","    alea = np.random.random()\r\n","    if valeur > seuil:\r\n","        res.append(1)\r\n","    elif (valeur < seuil) & (data_nb1[i] == 0) & (data_nb2[i] == 0):\r\n","        res.append(0)\r\n","    elif (valeur < seuil) & (data_ratio1[i] < 0.0002) & (data_ratio2[i] < 0.0005):\r\n","        res.append(0)\r\n","    else:\r\n","        if valeur > seuil / 2 and alea > score_to_threshold((seuil - valeur)):\r\n","            res.append(1)\r\n","        elif valeur < seuil / 2 and alea > score_to_threshold(valeur):\r\n","            res.append(0)\r\n","        else:\r\n","            res.append('?')\r\n","\r\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"64i8Hhu3RkY5"},"source":["# Function to tell if a document talk about management range or not\r\n","def gestion(data_nb1: pd.DataFrame, data_nb2: pd.DataFrame, data_nb3: pd.DataFrame, data_ratio1: pd.DataFrame, data_ratio2: pd.DataFrame, data_ratio3 : pd.DataFrame) -> list:\r\n","    \"\"\"Documentation\r\n","    this function calculate a score who represents if a document talk about management range or not.\r\n","    The higher score show document who talk about management range and the weakest show document who don't talk about management range.\r\n","    We define a threshold and the document who have a score higher of this threshold take a 1 who represent management range.\r\n","    Documents whith little score are consider such as they don't talk about management range take 0.\r\n","    The others have a '?' because we don't know if they talk about management range or not and we try in the rest of notebooks to create labels\r\n","    for the documents with '?' thanks to documents who have labels\r\n","    Parameters:\r\n","          data_nb1: First column of a dataframe who describe a number of key words presents in text\r\n","          data_nb2: Second column of a dataframe who describe a number of key words presents in title\r\n","          data_nb3: Third column who describe the distinct number of key word presents in text\r\n","          data_ratio1: Linked column of data_nb1 who represents a ratio of key word apparition in text\r\n","          compared to the total number of words in text\r\n","          data_ratio2: Linked column of data_nb2 who represents a ratio of key word apparition in title\r\n","          compared to the total number of words in title\r\n","          data_ratio3: Linked column of data_nb3 who represents a ratio of distinct key word apparition in text\r\n","          compared to the total number of distinct word in text\r\n","\r\n","    Out:\r\n","          list: represents whether a document is innovative or not (i.e. yes = 1, no = 0, don't know = ?)\r\n","    \"\"\"\r\n","  res: list = []\r\n","  seuil: int = 0.70\r\n","\r\n","  for i in tqdm(range(len(data_nb1))):\r\n","    valeur = 0.5*(data_nb1[i]*data_ratio1[i]) + 0.5*(data_nb2[i]*data_ratio2[i]) + (data_nb3[i]*data_ratio3[i])\r\n","    alea = np.random.random()\r\n","    if valeur > seuil:\r\n","        res.append(1)\r\n","    elif (valeur < seuil) & (data_nb1[i] == 0) & (data_nb2[i] == 0):\r\n","        res.append(0)\r\n","    elif (valeur < seuil) & (data_ratio1[i] < 0.0002) & (data_ratio2[i] < 0.0005):\r\n","        res.append(0)\r\n","    else:\r\n","        if valeur > seuil / 2 and alea > score_to_threshold((seuil - valeur)):\r\n","            res.append(1)\r\n","        elif valeur < seuil / 2 and alea > score_to_threshold(valeur):\r\n","            res.append(0)\r\n","        else:\r\n","            res.append('?')\r\n","\r\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5TZ4u31RkUj"},"source":["# Function use to create all features we need\r\n","def create_features(df : pd.DataFrame, texte : str, title : str, inno_ges : str, df_lexique : pd.DataFrame) -> pd.DataFrame:\r\n","  \"\"\"Documentation\r\n","    Parameter:\r\n","        df: A dataframe on which we will create features\r\n","        texte : Name of column who contain text\r\n","        title : Name of column who contain title of article\r\n","        inno_ges : String to specify if we want an analyse on management range or innovation.\r\n","        We can only choose options \"innovation\" or \"gestion\"\r\n","  \"\"\"\r\n","  df[\"nb_key_words\"]: pd.DataFrame = count_key_words(df[texte], df_lexique[\"key_words_lemma\"])\r\n","  df[\"nb_key_words_title\"]: pd.DataFrame = count_key_words(df[title], df_lexique[\"key_words_lemma\"])\r\n","  df[\"nb_words\"]: pd.DataFrame = nb_word(df[texte])\r\n","  df[\"nb_words_title\"]: pd.DataFrame = nb_word(df[title])\r\n","  df[\"nb_sentences\"]: pd.DataFrame  = phrases(df, texte)\r\n","  df[\"average_word_sentence\"]: pd.DataFrame = df[\"nb_words\"] / df[\"nb_sentences\"]\r\n","  df[\"ratio_word_title_on_word\"]: pd.DataFrame  = df[\"nb_words_title\"] / df[\"nb_words\"]\r\n","  df['ratio_key_words']: pd.DataFrame  = df['nb_key_words']/df['nb_words']\r\n","  df['ratio_key_words']: pd.DataFrame  = df['ratio_key_words'].fillna(0)\r\n","  df['ratio_key_sentences']: pd.DataFrame  = df['nb_key_words']/df['nb_sentences']\r\n","  df['ratio_key_sentences']: pd.DataFrame  = df['ratio_key_sentences'].fillna(0)\r\n","  df['ratio_key_word_title']: pd.DataFrame  = df['nb_key_words_title'] / df['nb_words_title']\r\n","  df['ratio_key_word_title']: pd.DataFrame  = df['ratio_key_word_title'].fillna(0)\r\n","  df['word_key_diff']: pd.DataFrame = key_word_in_doc(df[texte].tolist(), df_lexique['key_words_lemma'])\r\n","  df['word_diff']: pd.DataFrame = count_words_diff(df[texte].tolist(), df_lexique['key_words_lemma'])\r\n","  df['ratio_key_word_diff']: pd.DataFrame  = df['word_key_diff'] / df['word_diff']\r\n","  \r\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILJXday4RkSN"},"source":["# Function use for create a label for each documents\r\n","def create_label(df : pd.DataFrame, inno_ges : str, df_lexique : pd.DataFrame) -> pd.DataFrame:\r\n","  \"\"\"Documentation\r\n","    Parameter:\r\n","        df: A dataframe who contains article that we are going to classify\r\n","        inno_ges: String to specify if we want an analyse on management range or innovation.\r\n","        We can only choose options \"innovation\" or \"gestion\"\r\n","        df_lexique : A daframe with the lexique of gestion or innovation\r\n","  \"\"\"\r\n","  df = create_features(df, \"art_content_clean_without_lem\", \"art_title\", inno_ges, df_lexique)\r\n","\r\n","  if (inno_ges == 'innovation'):\r\n","    df['prediction_supervise'] = innovation(df['nb_key_words'].tolist(),df['nb_key_words_title'].tolist(),df['word_key_diff'].tolist(), df['ratio_key_words'].tolist(),df['ratio_key_word_title'].tolist(), df['ratio_key_word_diff'].tolist())\r\n","  elif (inno_ges == 'gestion'):\r\n","    df['prediction_supervise'] = gestion(df['nb_key_words'].tolist(),df['nb_key_words_title'].tolist(),df['word_key_diff'].tolist(), df['ratio_key_words'].tolist(),df['ratio_key_word_title'].tolist(), df['ratio_key_word_diff'].tolist())\r\n","\r\n","  var_utile = df.drop([\"art_id\",\"art_content_clean_without_lem\",\"art_title\",\"prediction_supervise\"], axis = 1)\r\n","  var_utile = var_utile.drop([\"nb_key_words\",\"nb_key_words_title\",\"nb_words\",\"nb_words_title\",\"nb_sentences\",\"average_word_sentence\",\"ratio_word_title_on_word\",'ratio_key_words','ratio_key_sentences','ratio_key_word_title','word_key_diff','word_diff','ratio_key_word_diff'],axis = 1)\r\n","\r\n","  var_utile = pd.concat([var_utile, df['prediction_supervise']], axis = 1)\r\n","\r\n","  tout = var_utile[var_utile['prediction_supervise'] != '?']\r\n","  unlabeled = var_utile[var_utile['prediction_supervise'] == '?']\r\n","\r\n","  X_train = tout.drop('prediction_supervise', axis=1)\r\n","  y_train = tout.prediction_supervise\r\n","  y_train = pd.to_numeric(y_train)\r\n","\r\n","  X_unlabeled = unlabeled.drop('prediction_supervise', axis=1)\r\n","\r\n","  if (inno_ges == 'innovation'):\r\n","    model1 = XGBClassifier(random_state=0)\r\n","    model2 = DecisionTreeClassifier(random_state=0)\r\n","    model3 = LogisticRegression(penalty = 'l1', solver = 'liblinear',random_state=0)\r\n","  elif (inno_ges == 'gestion'):\r\n","    model1 = XGBClassifier(random_state=0)\r\n","    model2 = LogisticRegression(penalty = 'l1', solver = 'liblinear',random_state=0)\r\n","    model3 = svm.SVC(C = 4,kernel='linear', probability= True,random_state=0)\r\n","  # Initiate iteration counter\r\n","  iterations = 0\r\n","\r\n","  # Containers to hold f1_scores and # of pseudo-labels\r\n","  train_f1s = []\r\n","  pseudo_labels = []\r\n","\r\n","  # Assign value to initiate while loop\r\n","  high_prob = [1] \r\n","\r\n","  # Loop will run until there are no more high-probability pseudo-labels\r\n","  while len(high_prob) > 0:\r\n","        \r\n","    # Fit classifier and make train/test predictions\r\n","    model1.fit(X_train, y_train)\r\n","    y_hat_train = model1.predict(X_train)\r\n","\r\n","    # Calculate and print iteration # and f1 scores, and store f1 scores\r\n","    train_f1 = f1_score(y_train, y_hat_train)\r\n","    print(f\"Iteration {iterations}\")\r\n","    print(f\"Train f1: {train_f1}\")\r\n","    train_f1s.append(train_f1)\r\n","   \r\n","    if (len(X_unlabeled) > 0):\r\n","      # Generate predictions and probabilities for unlabeled data\r\n","      print(f\"Now predicting labels for unlabeled data...\")\r\n","\r\n","      pred_probs : np.ndarray = model1.predict_proba(X_unlabeled)\r\n","      preds : np.ndarray = model1.predict(X_unlabeled)\r\n","      prob_0 : list = pred_probs[:,0]\r\n","      prob_1 : list = pred_probs[:,1]\r\n","\r\n","      # Store predictions and probabilities in dataframe\r\n","      df_pred_prob : pd.DataFrame = pd.DataFrame([])\r\n","      df_pred_prob['preds'] = preds\r\n","      df_pred_prob['prob_0'] = prob_0\r\n","      df_pred_prob['prob_1'] = prob_1\r\n","      df_pred_prob.index = X_unlabeled.index\r\n","    \r\n","      # Separate predictions with > 99% probability\r\n","      high_prob : pd.DataFrame = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.99],\r\n","                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.99]],\r\n","                          axis=0)\r\n","      print(f\"{len(high_prob)} high-probability predictions added to training data.\")\r\n","    \r\n","      pseudo_labels.append(len(high_prob))\r\n","\r\n","      # Add pseudo-labeled data to training data\r\n","      X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\r\n","      y_train = pd.concat([y_train, high_prob.preds])      \r\n","    \r\n","      # Drop pseudo-labeled instances from unlabeled data\r\n","      X_unlabeled = X_unlabeled.drop(index=high_prob.index)\r\n","      print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\r\n","    \r\n","      # Update iteration counter\r\n","      iterations += 1\r\n","    else :\r\n","      high_prob = []\r\n","      print(f'end of process.')\r\n","\r\n","  # Initiate iteration counter\r\n","  iterations = 0\r\n","\r\n","  # Containers to hold f1_scores and # of pseudo-labels\r\n","  train_f1s = []\r\n","  pseudo_labels = []\r\n","\r\n","  # Assign value to initiate while loop\r\n","  high_prob = [1] \r\n","\r\n","  # Loop will run until there are no more high-probability pseudo-labels\r\n","  while len(high_prob) > 0:\r\n","        \r\n","    # Fit classifier and make train/test predictions\r\n","    model2.fit(X_train, y_train)\r\n","    y_hat_train = model2.predict(X_train)\r\n","\r\n","    # Calculate and print iteration # and f1 scores, and store f1 scores\r\n","    train_f1 = f1_score(y_train, y_hat_train)\r\n","    print(f\"Iteration {iterations}\")\r\n","    print(f\"Train f1: {train_f1}\")\r\n","    train_f1s.append(train_f1)\r\n","   \r\n","    if (len(X_unlabeled) > 0):\r\n","      # Generate predictions and probabilities for unlabeled data\r\n","      print(f\"Now predicting labels for unlabeled data...\")\r\n","\r\n","      pred_probs : np.ndarray = model2.predict_proba(X_unlabeled)\r\n","      preds : np.ndarray = model2.predict(X_unlabeled)\r\n","      prob_0 : list = pred_probs[:,0]\r\n","      prob_1 : list = pred_probs[:,1]\r\n","\r\n","      # Store predictions and probabilities in dataframe\r\n","      df_pred_prob : pd.DataFrame = pd.DataFrame([])\r\n","      df_pred_prob['preds'] = preds\r\n","      df_pred_prob['prob_0'] = prob_0\r\n","      df_pred_prob['prob_1'] = prob_1\r\n","      df_pred_prob.index = X_unlabeled.index\r\n","    \r\n","      # Separate predictions with > 99% probability\r\n","      high_prob : pd.DataFrame = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.99],\r\n","                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.99]],\r\n","                          axis=0)\r\n","      print(f\"{len(high_prob)} high-probability predictions added to training data.\")\r\n","    \r\n","      pseudo_labels.append(len(high_prob))\r\n","\r\n","      # Add pseudo-labeled data to training data\r\n","      X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\r\n","      y_train = pd.concat([y_train, high_prob.preds])      \r\n","    \r\n","      # Drop pseudo-labeled instances from unlabeled data\r\n","      X_unlabeled = X_unlabeled.drop(index=high_prob.index)\r\n","      print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\r\n","    \r\n","      # Update iteration counter\r\n","      iterations += 1\r\n","    else :\r\n","      high_prob = []\r\n","      print(f'end of process.')\r\n","\r\n","  # Initiate iteration counter\r\n","  iterations = 0\r\n","\r\n","  # Containers to hold f1_scores and # of pseudo-labels\r\n","  train_f1s = []\r\n","  pseudo_labels = []\r\n","\r\n","  # Assign value to initiate while loop\r\n","  high_prob = [1] \r\n","\r\n","  # Loop will run until there are no more high-probability pseudo-labels\r\n","  while len(high_prob) > 0:\r\n","        \r\n","    # Fit classifier and make train/test predictions\r\n","    model3.fit(X_train, y_train)\r\n","    y_hat_train = model3.predict(X_train)\r\n","\r\n","    # Calculate and print iteration # and f1 scores, and store f1 scores\r\n","    train_f1 = f1_score(y_train, y_hat_train)\r\n","    print(f\"Iteration {iterations}\")\r\n","    print(f\"Train f1: {train_f1}\")\r\n","    train_f1s.append(train_f1)\r\n","   \r\n","    if (len(X_unlabeled) > 0):\r\n","      # Generate predictions and probabilities for unlabeled data\r\n","      print(f\"Now predicting labels for unlabeled data...\")\r\n","\r\n","      pred_probs : np.ndarray = model3.predict_proba(X_unlabeled)\r\n","      preds : np.ndarray = model3.predict(X_unlabeled)\r\n","      prob_0 : list = pred_probs[:,0]\r\n","      prob_1 : list = pred_probs[:,1]\r\n","\r\n","      # Store predictions and probabilities in dataframe\r\n","      df_pred_prob : pd.DataFrame = pd.DataFrame([])\r\n","      df_pred_prob['preds'] = preds\r\n","      df_pred_prob['prob_0'] = prob_0\r\n","      df_pred_prob['prob_1'] = prob_1\r\n","      df_pred_prob.index = X_unlabeled.index\r\n","    \r\n","      # Separate predictions with > 60% probability\r\n","      high_prob : pd.DataFrame = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.50],\r\n","                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.50]],\r\n","                          axis=0)\r\n","      print(f\"{len(high_prob)} high-probability predictions added to training data.\")\r\n","    \r\n","      pseudo_labels.append(len(high_prob))\r\n","\r\n","      # Add pseudo-labeled data to training data\r\n","      X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\r\n","      y_train = pd.concat([y_train, high_prob.preds])      \r\n","    \r\n","      # Drop pseudo-labeled instances from unlabeled data\r\n","      X_unlabeled = X_unlabeled.drop(index=high_prob.index)\r\n","      print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\r\n","    \r\n","      # Update iteration counter\r\n","      iterations += 1\r\n","    else :\r\n","      high_prob = []\r\n","      print(f'end of process.')\r\n","\r\n","  X_train['prediction_supervise'] = y_train\r\n","  X_train['index'] = X_train.index\r\n","  df[\"index\"] = df.index\r\n","  df_final = pd.merge(left=df, right=X_train, left_on='index', right_on='index')\r\n","  df_final = df_final[['art_id','prediction_supervise_y']]\r\n","  df_final = df_final.rename(columns = {'prediction_supervise_y': 'prediction_supervise'})\r\n","\r\n","  return df_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SyanKb0LoTIi"},"source":["#Function that compute score of innovation and the score of gestion.\r\n","#To make this score we run a cross validation and predict the proba on the test set. The proba on the test set is used to make the score of\r\n","#innovation and the score of gestion.\r\n","def compute_score(data: pd.DataFrame):\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","    data: Data used to compute the score of innovation and the score of gamme gestion\r\n","\r\n","  Out:\r\n","    data: DataFrame which contain the id, the score of innovation and the score of gamme gestion\r\n","  \"\"\"\r\n","  data.reset_index(drop=True,inplace = True)\r\n","\r\n","  #Creation of the kfold for split train and test\r\n","  skf = StratifiedKFold(n_splits=5, random_state = 42, shuffle=True)\r\n","  data[\"score_innovant\"] = 0\r\n","  data[\"score_gamme_gestion\"] = 0\r\n","\r\n","  #We create cross validation to predict the proba on test set (and use this proba as score)\r\n","  for train_index, test_index in skf.split(data, data[\"innovant\"]):\r\n","    #Initialisation of the model\r\n","    model = RandomForestClassifier(random_state=42)\r\n","    #We train on train set\r\n","    model.fit(data.drop([\"innovant\",\"gamme_gestion\",\"art_id\",\"score_innovant\",\"score_gamme_gestion\"],axis = 1).loc[train_index],data.loc[train_index,\"innovant\"])\r\n","    #We predict on test set and we use the proba make the decision score\r\n","    data.loc[test_index,\"score_innovant\"] = model.predict_proba(data.drop([\"innovant\",\"gamme_gestion\",\"art_id\",\"score_innovant\",\"score_gamme_gestion\"],axis = 1).loc[test_index])[:,1]\r\n","\r\n","  #We create cross validation to predict the proba on test set (and use this proba as score)\r\n","  for train_index, test_index in skf.split(data, data[\"innovant\"]):\r\n","    #Initialisation of the model\r\n","    model = RandomForestClassifier(random_state=42)\r\n","    #We train on train set\r\n","    model.fit(data.drop([\"innovant\",\"gamme_gestion\",\"art_id\",\"score_innovant\",\"score_gamme_gestion\"],axis = 1).loc[train_index],data.loc[train_index,\"gamme_gestion\"])\r\n","    #We predict on test set and we use the proba make the decision score\r\n","    data.loc[test_index,\"score_gamme_gestion\"] = model.predict_proba(data.drop([\"innovant\",\"gamme_gestion\",\"art_id\",\"score_innovant\",\"score_gamme_gestion\"],axis = 1).loc[test_index])[:,1]\r\n","\r\n","  #We round the score of innovation and gamme gestion\r\n","  data[\"score_innovant\"] = np.round(data[\"score_innovant\"],3)\r\n","  data[\"score_gamme_gestion\"] = np.round(data[\"score_gamme_gestion\"],3)\r\n","\r\n","  return data[[\"art_id\",\"score_innovant\",\"score_gamme_gestion\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOEdbN5CuNyT"},"source":["#Function that train the model\r\n","def train_model(data: pd.DataFrame, path_to: str):\r\n","  '''Documentation\r\n","  Parameters:\r\n","        data : data to train the model\r\n","        data_path : path to save data\r\n","  '''\r\n","  #Initialisation of the model\r\n","  model = RandomForestClassifier(random_state=42)\r\n","  #Train model\r\n","  model.fit(data.drop([\"innovant\",\"gamme_gestion\"], axis = 1),data[\"innovant\"])\r\n","  #Save the model\r\n","  pickle.dump(model, open(path_to + \"model_innovant.pkl\", 'wb'))\r\n","\r\n","  #Initialisation of the model\r\n","  model = RandomForestClassifier(random_state=42)\r\n","  #Train model\r\n","  model.fit(data.drop([\"innovant\",\"gamme_gestion\"], axis = 1),data[\"gamme_gestion\"])\r\n","  #Save the model\r\n","  pickle.dump(model, open(path_to + \"model_gamme_gestion.pkl\", 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"evrqzi4X3Xks"},"source":["# Detection New Documents"]},{"cell_type":"code","metadata":{"id":"SGKRD1Lp3W4b"},"source":["#Function who take a dataframe, normalize the data, run a tsne on 3 components and run an isolation forest.\r\n","#The purpose of this function is to predict the new documents\r\n","def duplicate_prediction(data: pd.DataFrame, path_save: str) -> pd.DataFrame:\r\n","    \"\"\"Documentation\r\n","    Parameters:\r\n","        data_path : path to data\r\n","    \r\n","    Out:\r\n","        prediction : list of prediction (-1 : News, 1: Common)\r\n","    \"\"\"\r\n","    #We create the dataframe that will save the data\r\n","    dataout = pd.DataFrame(data[\"art_id\"].values,columns = [\"art_id\"])\r\n","    data = data.drop([\"art_id\"],axis = 1)\r\n","\r\n","    #We normalize the data\r\n","    sc: StandardScaler = StandardScaler()\r\n","    X: np.ndarray = sc.fit_transform(data)\r\n","    pickle.dump(sc, open(path_save + \"scaler.pkl\", 'wb'))\r\n","\r\n","    #We run TSNE\r\n","    tsne: TSNE = TSNE(n_components=3)\r\n","    X: np.ndarray = tsne.fit_transform(X)\r\n","    pickle.dump(tsne, open(path_save + \"tsne.pkl\", 'wb'))\r\n","\r\n","    #We run isolation forest\r\n","    clf: IsolationForest = IsolationForest(random_state=0, contamination=0.01)\r\n","    dataout[\"prediction_nouveau\"]: np.ndarray = clf.fit_predict(X)\r\n","    dataout[\"score_nouveau\"]: np.ndarray = clf.decision_function(X)\r\n","    pickle.dump(clf, open(path_save + \"model_novelty.pkl\", 'wb'))\r\n","\r\n","    return dataout"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GMBKujOiQ8Sy"},"source":["# Détection de clusters"]},{"cell_type":"code","metadata":{"id":"07HA6wNCQ8Co"},"source":["# Find the optimal number of clusters, k\r\n","def optimal_cluster(X: pd.DataFrame)->int:\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      X: dataset containint all our datas we want to train the model with\r\n","\r\n","  Out:\r\n","      (list_k[np.argmax(list_error_silhouette)]): optimal value of clusters, k\r\n","  \"\"\"\r\n","  #We create a list to stock the silhouette\r\n","  list_error_silhouette:list = []\r\n","  #We create a list to stock the k\r\n","  list_k:list = []\r\n","  #The range of k\r\n","  K:range = range(9,26)\r\n","  #We create the standart scaler to normalize the data\r\n","  sc: StandardScaler = StandardScaler()\r\n","  X: np.ndarray = sc.fit_transform(X)\r\n","\r\n","  #create the range of k which are going to be used in our loop\r\n","  for k in tqdm_notebook(K):\r\n","    #We compute pca\r\n","    pca: PCA = PCA(n_components=10,random_state = 1)\r\n","    #We run kmenoid with k clusters\r\n","    km = KMedoids(n_clusters=k,metric='cosine',  random_state=0)\r\n","    y = km.fit_predict(pca.fit_transform(X))\r\n","    list_k.append(k)\r\n","    #create a list with all k values\r\n","    list_error_silhouette.append(silhouette_score(X,y))\r\n","    #return the maximum of the error based on the silhouette corresponding to the optimal value of k\r\n","\r\n","  return (list_k[np.argmax(list_error_silhouette)])\r\n","\r\n","\r\n","# Train the chosen model on X matrix with the chosen number of clusters (nb_cluster) and return the trained model for later use\r\n","def training(X: pd.DataFrame, nb_cluster: int, path_save: str)->(KMedoids, PCA, StandardScaler):\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      X: The data to predict\r\n","      nb_cluster: The number of cluster\r\n","      path_save: The path to save the model\r\n","\r\n","  Out:\r\n","      km: The model\r\n","      pca: The pca\r\n","      sc: The normalisation\r\n","  \"\"\"\r\n","  #Nomalisation\r\n","  sc: StandardScaler = StandardScaler()\r\n","  X: np.ndarray = sc.fit_transform(X)\r\n","  pickle.dump(sc, open(path_save + \"scaler_cluster.pkl\", 'wb'))\r\n","\r\n","  #PCA\r\n","  pca: PCA = PCA(n_components=10,random_state = 1)\r\n","  X: np.ndarray = pca.fit_transform(X)\r\n","  pickle.dump(pca, open(path_save + \"pca_cluster.pkl\", 'wb'))\r\n","\r\n","  #Kmenoids\r\n","  km: KMedoids = KMedoids(n_clusters=nb_cluster,metric='cosine',  random_state=0).fit(X)\r\n","  pickle.dump(km, open(path_save + \"model_cluster.pkl\", 'wb'))\r\n","\r\n","  return km, pca, sc\r\n","\r\n","#We take a dataframe and we return prediction\r\n","def predict(X: pd.DataFrame,model: KMedoids,pca: PCA,sc: StandardScaler)->list:\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      X: The data to predict\r\n","      km: The model\r\n","      pca: The pca to make before to run the model\r\n","      sc: The normalisation to make before to run the pca\r\n","\r\n","  Out:\r\n","      return prediction\r\n","  \"\"\"\r\n","  #Normalisation\r\n","  X: np.ndarray = sc.transform(X)\r\n","  #pca\r\n","  X: np.ndarray = pca.transform(X)\r\n","  \r\n","  #prediction\r\n","  return model.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsJfPoQeQ8Aq"},"source":["#calculate the chosen distance (euclidean or cosine) between the values inside\r\n","#the cluster with label denomination and the centroid of this cluster.\r\n","def compute_distance(v: np.ndarray,km: KMedoids,label: int,pca: PCA,sc: StandardScaler)->list:\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      v: The article to compute\r\n","      km: The model\r\n","      label: The number of the cluster\r\n","      pca: The pca to make before to run the model\r\n","      sc: The normalisation to make before to run the pca\r\n","\r\n","  Out:\r\n","      distance bewteen the point and the center of the cluster\r\n","  \"\"\"\r\n","  #We make the euclidian distance between the point and the center of the cluster\r\n","  return distance.euclidean(pca.transform(sc.transform(v.reshape(1, -1))),km.cluster_centers_[label])\r\n","\r\n","#We give a list of article and return the 3 most recurents word\r\n","def obtain_word(list_content: list, type = \"idf\")-> str:\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      X: contains the data\r\n","\r\n","  Out:\r\n","      (list_k[np.argmax(list_error_silhouette)]): optimal value of clusters, k\r\n","  \"\"\"\r\n","  if type == \"idf\":\r\n","    #We drop english word because there is some english word in the content\r\n","    vectorizer = TfidfVectorizer(stop_words = \"english\")\r\n","    X = vectorizer.fit_transform(list_content)\r\n","    #Vectorizes all the tags in the train set while cleaning it from stop words\r\n","  elif type == \"bow\":\r\n","    #We drop english word because there is some english word in the content\r\n","    vectorizer = CountVectorizer(stop_words = \"english\")\r\n","    X = vectorizer.fit_transform(list_content)\r\n","    #Vectorizes all the tags in the train set while cleaning it from stop words\r\n","  else:\r\n","    print(\"None\")\r\n","    #return an error while the type specified is not idf or bow\r\n","\r\n","  #We create a dataframe with the sum of the bow/td-idf and we take the 3 word with the biggest score\r\n","  temp_data = pd.DataFrame(np.array(np.sum(X,axis = 0))[0], columns = [\"score\"])\r\n","  temp_data[\"word\"] = vectorizer.get_feature_names()\r\n","  #Create a word column from feature integer indices to feature name.\r\n","  temp_data = temp_data.sort_values(by = [\"score\"], ascending = False)\r\n","  return temp_data.iloc[0,1] + \"/\" + temp_data.iloc[1,1] + \"/\" + temp_data.iloc[2,1]\r\n","\r\n","\r\n","# Get the content of an article except the words present in the whitelist\r\n","def whitelist_content(list_content: list, list_whitelist: list) -> np.ndarray:\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      list_content: A list of articles\r\n","      list_whitelist: A whitelist of word\r\n","\r\n","  Out:\r\n","      final_content: A list of articles without the word in the whitelist\r\n","  \"\"\"\r\n","  #We create a list\r\n","  final_content = []\r\n","\r\n","  for content in list_content:\r\n","    #We take every word witch are not present in whitelist\r\n","    temp_content = [w for w in content.split() if not w in list_whitelist]\r\n","    #We join all the word to create a sentence\r\n","    temp_content = \" \".join(temp_content)\r\n","    final_content.append(temp_content)\r\n","\r\n","  return np.array(final_content)\r\n","\r\n","\r\n","#We compute the distance between the centroide of each cluster and all the point of this cluster\r\n","#we take the 5 closest point of the cluster and we use this 5 articles to select the name of the cluster.\r\n","#To select the name of the cluster we take the 3 most recurrents words.\r\n","def get_label_title(data: pd.DataFrame,whitelist: list,km: KMedoids,pca: PCA,sc: StandardScaler, path_save: str) -> dict:\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      data: A dataframe containing all our datas we want to work with\r\n","      whitelist: a whitelist of words we want to avoid in labels title\r\n","\r\n","  Out:\r\n","      themes: A string containing our title of the chosen label\r\n","  \"\"\"\r\n","  #We create the dictionnary that will contain the label corresponding to each cluster\r\n","  themes = {}\r\n","\r\n","  for label in np.unique(data[\"prediction\"]):\r\n","    #We create a copy of the dataframe which only contain the cluster corresponding to the value of label\r\n","    sub_data = data.query(\"prediction == @label\").drop([\"prediction\"],axis = 1).copy()\r\n","    #We compute the distance bewteen the center of the cluster and all the point of the cluster\r\n","    sub_data[\"score\"] = sub_data.drop([\"art_id\",\"art_content_clean_without_lem\"], axis = 1).apply(compute_distance,axis = 1,raw = True, args = (km,0,pca,sc))\r\n","    #We take the five closest document of the center\r\n","    head_content = sub_data.sort_values(by = [\"score\"], ascending = True).head(5)[\"art_content_clean_without_lem\"].values\r\n","    #We drop the word in the whitelist\r\n","    head_content = whitelist_content(head_content,whitelist)\r\n","    #We get the label of ech cluster by checking the three recurents words\r\n","    themes[label] = obtain_word(head_content, type = \"bow\")\r\n","    #We save the label in a dictionnary\r\n","    pickle.dump(themes, open(path_save + \"dict_label.pkl\", 'wb'))\r\n","    \r\n","  return themes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eCh5C7-7Q7-U"},"source":["# Clusters the dataset with the optimal number of clusters and return\r\n","def predict_cluster(data: pd.DataFrame, path_save: str)-> pd.DataFrame:\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      data: A dataframe containing all our datas we want to work with\r\n","      \r\n","  Out:\r\n","      data[[\"art_id\",\"prediction\"]]: A dataframe containing our id and the title of the chosen label\r\n","      model: the model trained with the optimal number of clusters\r\n","  \"\"\"\r\n","  #We use the function higher to obtain the good number of clusters\r\n","  nb_cluster = optimal_cluster(data.drop([\"art_id\",\"art_content_clean_without_lem\"], axis = 1))\r\n","  print(\"We take \" + str(nb_cluster) + \" clusters.\")\r\n","  #We train the kmenoide, pca and normalisation to make prediction\r\n","  model, pca, sc = training(data.drop([\"art_id\",\"art_content_clean_without_lem\"], axis = 1),nb_cluster, path_save)\r\n","  #We predict the cluster with the kmenoide, pca and normalisation trained\r\n","  data[\"prediction\"] = predict(data.drop([\"art_id\",\"art_content_clean_without_lem\"], axis = 1),model, pca, sc)\r\n","  #We create a whitelist to drop some recurents word that was not drop\r\n","  whitelist = [\"plus\",\"tout\",\"cette\",\"ca\",\"etre\",\"dire\",\"faut\",\"fait\",\"faire\",\"donc\",\"avoir\",\"être\",\"leur\",\"le\",\"mettre\",\"pouvoir\",\"art_id\",\"entreprise\",\"gestion\",\"entreprises\",\"aussi\",\"meme\",\"bien\",\"ete\",\"comme\",\"etat\",\"ministre\",\"tres\",\"encore\",\"peut\",\"dont\",\"egalement\",\"notamment\",\"ainsi\",\"leurs\",\"entre\"]\r\n","  #We create the label of each cluster\r\n","  data[\"name_theme\"] = data[\"prediction\"].map(get_label_title(data,whitelist,model,pca,sc,path_save))\r\n","\r\n","  return data[[\"art_id\",\"name_theme\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cP1b7BclQ4zP"},"source":["#Big function wich call all the other function"]},{"cell_type":"code","metadata":{"id":"rCBJeeIi4K4L"},"source":["def all_prediction(data: pd.DataFrame,lexique_innovant: pd.DataFrame,lexique_gestion: pd.DataFrame,path_save: str) -> pd.DataFrame:\r\n","  \"\"\"Documentation\r\n","  Parameters:\r\n","      data: A dataframe containing 3 columns(art_id, art_content, art_uitle) and the bow\r\n","  Output:\r\n","      data: A dataframe with all the the predicted features\r\n","  \"\"\"\r\n","  np.random.seed(42)\r\n","  #We compute innovant and gamme gestion\r\n","  print(\"Phase 1\")\r\n","  #We predict the label innovant\r\n","  data_temp_innovant = create_label(data.copy(),\"innovation\",lexique_innovant)\r\n","  data_temp_innovant.columns = [\"art_id\",\"innovant\"]\r\n","  #We predict the label gestion\r\n","  data_temp_gestion = create_label(data.copy(),\"gestion\",lexique_gestion)\r\n","  data_temp_gestion.columns = [\"art_id\",\"gamme_gestion\"]\r\n","  #We make a concatenation bewteen the input data and the predicted label\r\n","  data = data.merge(data_temp_innovant, on = \"art_id\").merge(data_temp_gestion, on = \"art_id\")\r\n","  data.rename(columns = {\"innovant_y\" : \"innovant\",\"gamme_gestion_y\" : \"gamme_gestion\"}, inplace = True)\r\n","\r\n","  #We compute the score\r\n","  print(\"Phase 2\")\r\n","  #We predict the score of innovant and gamme gestion\r\n","  data_temp = compute_score(data.drop([\"art_content_clean_without_lem\",\"art_title\"], axis = 1).copy())\r\n","  data = data.merge(data_temp, on = \"art_id\")\r\n","  #We make a concatenation bewteen the input data and the predicted label\r\n","  data.rename(columns = {\"score_innovant_y\" : \"score_innovant\",\"score_gamme_gestion_y\" : \"score_gamme_gestion\"}, inplace = True)\r\n","  #We train model\r\n","  train_model(data.drop([\"art_id\",\"art_content_clean_without_lem\",\"score_innovant\",\"score_gamme_gestion\",\"art_title\"], axis = 1).copy(),path_save)\r\n","\r\n","  #We compute the new documents\r\n","  print(\"Phase 3\")\r\n","  #We predict the new document\r\n","  data_temp = duplicate_prediction(data.query(\"innovant == 1\").query(\"gamme_gestion == 1\").drop([\"art_content_clean_without_lem\",\"art_title\",\"score_innovant\",\"score_gamme_gestion\",\"innovant\",\"gamme_gestion\"], axis = 1).copy(),path_save)\r\n","  #We make a concatenation bewteen the input data and the predicted label\r\n","  data_temp[\"prediction_nouveau\"] = data_temp[\"prediction_nouveau\"].replace([1,-1],[0,1])\r\n","  data = data.merge(data_temp,how = \"left\", on = \"art_id\")\r\n","  data.rename(columns = {\"score_nouveau\" : \"score_nouveau\"}, inplace = True)\r\n","\r\n","  #On predict the cluster\r\n","  print(\"Phase 4\")\r\n","  #We run the function who predict the number of cluster\r\n","  data_temp = predict_cluster(data.query(\"innovant == 1\").query(\"gamme_gestion == 1\").drop([\"innovation\",\"gamme_gestion\",\"score_innovant\",\"score_gamme_gestion\",\"prediction_nouveau\",\"score_nouveau\",\"art_title\"], axis = 1).copy(),path_save)\r\n","  #We join the data\r\n","  data = data.merge(data_temp,how = \"left\", on = \"art_id\")\r\n","  #We select the columns that we want to give in output \r\n","  data = data[[\"art_id\",\"innovant\",\"gamme_gestion\",\"score_innovant\",\"score_gamme_gestion\",\"prediction_nouveau\",\"score_nouveau\",\"name_theme\"]].copy()\r\n","  #We rename some columns\r\n","  data.rename(columns = {\"name_theme\" : \"theme\",\"prediction_nouveau\" : \"nouveau\"}, inplace = True)\r\n","\r\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yanczv0AR16w"},"source":["# We run the function"]},{"cell_type":"code","metadata":{"id":"rzH5Tw0t4Wgb"},"source":["#We load the data who contain the BOW\r\n","data = pd.read_json(\r\n","    \"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Input/g3_BOW_v1.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fgqky6Z2Qjgw"},"source":["#We load the data who contain the title\r\n","data_title = pd.read_csv(\r\n","    '/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Input/Data_With_Features_Syntax.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6IkzEcybFMn"},"source":["#We load the data whith lemmatization on the content\r\n","data_lemma = pd.read_json(\r\n","    '/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Output/Innovation/df_articles_lemma.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONlQnNgTqKNs"},"source":["#We load the lexique of innovation\r\n","df_lexique_innovation: pd.DataFrame = pd.read_json(\r\n","    \"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Output/Innovation/df_lexique_lemma.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbpQQY9jQjoE"},"source":["#We load the lexique of gestion\r\n","df_lexique_gestion: pd.DataFrame = pd.read_json(\r\n","    \"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Output/Innovation/df_lexique_gammes_gestion.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38LSzfwPXrP2"},"source":["#We concatene all the data\r\n","data[\"art_title\"] = data_title.loc[data.index,\"art_title\"]\r\n","data[\"art_content_clean_without_lem\"] = data_lemma[\"art_lemma\"]\r\n","del(data_title)\r\n","del(data_lemma)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dq3xI0Z80jZz"},"source":["#We run the function who make all the prediction\r\n","output = all_prediction(data,df_lexique_innovation,df_lexique_gestion,\"/content/drive/MyDrive/G5 Inter-Promo 2021/Ressources/test/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ7FgbtlQjlw"},"source":["#We save the data\r\n","output.to_json(\"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Output/Global/Global_V2.json\",orient=\"records\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZxnvuLDXjAm"},"source":["output.shape"],"execution_count":null,"outputs":[]}]}