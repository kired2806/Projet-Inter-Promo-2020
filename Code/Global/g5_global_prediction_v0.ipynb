{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"g5_global_prediction_v0.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Z3WJ2JDgMPWU"},"source":["Created on Wednesday 15 January 2021\r\n","\r\n","**Group 5 - Classification**  \r\n","**Extraction features syntaxe**\r\n","\r\n","@authors : Jeremy Johann"]},{"cell_type":"markdown","metadata":{"id":"yn2EV2S5MMM-"},"source":["The notebook that contain all the code. You just need to put in the fonction the art_id, the content with lemmatization, the title of the article and the Bag Of Word (BOW). This code load all the model and give the prediction."]},{"cell_type":"markdown","metadata":{"id":"JxNRjNlcu28R"},"source":["# Pip install"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"97RchJsEb4oc","executionInfo":{"status":"ok","timestamp":1610980298262,"user_tz":-60,"elapsed":24940,"user":{"displayName":"Johann Jérémy","photoUrl":"","userId":"04957728469021807651"}},"outputId":"6b5db5aa-70bb-4a92-85c9-16bfb4080028"},"source":["pip install scikit-learn-extra"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting scikit-learn-extra\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/891d2ee7bd18af8f2e1df5d63a52ee96edd0eacc21bb9627072b1c5f6a6c/scikit-learn-extra-0.1.0b2.tar.gz (615kB)\n","\r\u001b[K     |▌                               | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 11.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 92kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 358kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 368kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 378kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 389kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 399kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 409kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 419kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 430kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 440kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 450kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 460kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 471kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 481kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 491kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 501kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 512kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 522kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 532kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 542kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 552kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 563kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 573kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 583kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 593kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 604kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 614kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 5.7MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->scikit-learn-extra) (1.0.0)\n","Building wheels for collected packages: scikit-learn-extra\n","  Building wheel for scikit-learn-extra (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-learn-extra: filename=scikit_learn_extra-0.1.0b2-cp36-cp36m-linux_x86_64.whl size=339565 sha256=1a668c1af2ce78a84a0219c9424756a5b8ca16254f6c7eb9c089106e451ae279\n","  Stored in directory: /root/.cache/pip/wheels/04/01/0f/943bffb48bac048fa216b4325f1a6c939491ccb0ff500e08f4\n","Successfully built scikit-learn-extra\n","Installing collected packages: scikit-learn-extra\n","Successfully installed scikit-learn-extra-0.1.0b2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t1tQ8GauvAjM"},"source":["# Import librairies"]},{"cell_type":"code","metadata":{"id":"Oy7uHDX9T0fS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610980300862,"user_tz":-60,"elapsed":27533,"user":{"displayName":"Johann Jérémy","photoUrl":"","userId":"04957728469021807651"}},"outputId":"37a1e18d-c724-463f-cabe-f6abb95f3622"},"source":["import re\r\n","import os\r\n","import nltk\r\n","import math\r\n","import string\r\n","import pickle\r\n","import warnings\r\n","import numpy as np\r\n","import pandas as pd\r\n","\r\n","from tqdm import tqdm_notebook\r\n","from tqdm import tqdm_notebook as tqdm\r\n","\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem import WordNetLemmatizer\r\n","from nltk.tokenize import sent_tokenize, word_tokenize\r\n","\r\n","from scipy.spatial import distance\r\n","\r\n","from xgboost import XGBClassifier\r\n","\r\n","from sklearn import svm\r\n","from sklearn.manifold import TSNE\r\n","from sklearn.cluster import KMeans\r\n","from sklearn.decomposition import PCA\r\n","from sklearn_extra.cluster import KMedoids\r\n","from sklearn.pipeline import make_pipeline\r\n","from sklearn.naive_bayes import CategoricalNB\r\n","from sklearn.tree import DecisionTreeClassifier\r\n","from sklearn.preprocessing import StandardScaler\r\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\r\n","from sklearn.model_selection import train_test_split, StratifiedKFold\r\n","from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor\r\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n","from sklearn.metrics import calinski_harabasz_score, silhouette_score, f1_score\r\n","from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, IsolationForest\r\n","\r\n","warnings.filterwarnings('ignore')\r\n","nltk.download('punkt')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"qiIIWDqN3Wik","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610980323052,"user_tz":-60,"elapsed":49709,"user":{"displayName":"Johann Jérémy","photoUrl":"","userId":"04957728469021807651"}},"outputId":"169ebbd2-012a-4f67-dc03-b692668a9d42"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LVA2R6Q7x06_"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"EZJVzxvHvrpt"},"source":["# Function who take in input the data and who give in output the novelty score and if the document is new or not\r\n","def prediction_nouveau(data: pd.DataFrame) -> pd.DataFrame:\r\n","    \"\"\"Documentation\r\n","    Parameters:\r\n","        data: Dataframe with the data to predit\r\n","    Out:\r\n","        prediction: list of prediction (-1 : News, 1: Common)\r\n","    \"\"\"\r\n","    # We create a dataframe where we will save the prediction\r\n","    dataout: pd.DataFrame = pd.DataFrame(data[\"art_id\"].values,columns = [\"art_id\"])\r\n","    data: pd.DataFrame = data.drop([\"art_id\"],axis = 1)\r\n","\r\n","    # We create the standard scaler that will normalize the data\r\n","    sc: StandardScaler = StandardScaler()\r\n","    X: np.ndarray = sc.fit_transform(data)\r\n","\r\n","    # We create the TSNE that will make a reduction dimension\r\n","    tsne: TSNE = TSNE(n_components=3)\r\n","    X: np.ndarray = tsne.fit_transform(X)\r\n","\r\n","    # We create model and we predict\r\n","    clf: IsolationForest = IsolationForest(random_state=0, contamination=0.01)\r\n","    dataout[\"prediction_nouveau\"]: np.ndarray = clf.fit_predict(X)\r\n","    dataout[\"score_nouveau\"]: np.ndarray = clf.decision_function(X)\r\n","\r\n","    # We return prediction\r\n","    return dataout"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N7EbqvalpTPC"},"source":["# Function who take in input the data and who give in output the score of innovation, the score of gestion, if the document is related to innovation and \r\n","#if the document is related to gestion\r\n","def prediction_innovant_gestion(data: pd.DataFrame, path_load: str) -> pd.DataFrame:\r\n","  \"\"\"Documentation\r\n","    Parameters:\r\n","        data: Dataframe with the data to predit\r\n","        path_load: path to the model to load\r\n","\r\n","    Out:\r\n","        dataout: Dataframe with the prediction\r\n","  \"\"\"\r\n","  # We create a dataframe where we will save the prediction\r\n","  dataout: pd.DataFrame = pd.DataFrame(data[\"art_id\"].values, columns = [\"art_id\"])\r\n","\r\n","  # We load the model\r\n","  model_innovant = pickle.load(open(path_load + \"model_innovant.pkl\", 'rb'))\r\n","  model_gestion = pickle.load(open(path_load + \"model_gamme_gestion.pkl\", 'rb'))\r\n","\r\n","  # We make the prediction and score of predition\r\n","  dataout[\"prediction_innovant\"] = model_innovant.predict(data.drop([\"art_id\"], axis = 1))\r\n","  dataout[\"score_innovant\"] = model_innovant.predict_proba(data.drop([\"art_id\"], axis = 1))[:,1]\r\n","  dataout[\"prediction_gamme_gestion\"] = model_gestion.predict(data.drop([\"art_id\"], axis = 1))\r\n","  dataout[\"score_gamme_gestion\"] = model_gestion.predict_proba(data.drop([\"art_id\"], axis = 1))[:,1]\r\n","\r\n","  # We return the prediction\r\n","  return dataout "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZD7R2JgnruWx"},"source":["# Function who take in input the data and who give in output the cluster of the document\r\n","def prediction_clustering(data: pd.DataFrame, path_load: str) -> pd.DataFrame:\r\n","  \"\"\"Documentation\r\n","    Parameters:\r\n","        data: Dataframe with the data to predit\r\n","        path_load: path to the model to load\r\n","    \r\n","    Out:\r\n","        dataout: Dataframe with the prediction\r\n","  \"\"\"\r\n","  # We create a dataframe where we will save the prediction\r\n","  dataout: pd.DataFrame = pd.DataFrame(data[\"art_id\"].values, columns = [\"art_id\"])\r\n","\r\n","  #We load the standart scaler to normalize the data\r\n","  sc = pickle.load(open(path_load + \"scaler_cluster.pkl\", 'rb'))\r\n","  #We load the pca\r\n","  pca = pickle.load(open(path_load + \"pca_cluster.pkl\", 'rb'))\r\n","  #We load the model\r\n","  model = pickle.load(open(path_load + \"model_cluster.pkl\", 'rb'))\r\n","\r\n","  #We predict\r\n","  X = sc.transform(data.drop([\"art_id\"], axis = 1))\r\n","  X = pca.transform(X)\r\n","  dataout[\"prediction_theme\"] = np.argmax(model.transform(X), axis=1)\r\n","\r\n","  #We return data\r\n","  return dataout"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCBJeeIi4K4L"},"source":["# Function who take in input the data and who give in output all the prediction\r\n","def all_prediction(data: pd.DataFrame,path_load: str) -> pd.DataFrame:\r\n","  '''Documentation\r\n","  Parameters:\r\n","      data: A dataframe containing 3 columns(art_id, art_content, art_uitle) and the bow\r\n","      path_load: path to the model to load\r\n","  \r\n","  Out:\r\n","      data : A dataframe with all the the predicted features\r\n","  '''\r\n","  # We compute innovant and gamme gestion\r\n","  print(\"Phase 1\")\r\n","  # We predict the document related to innovation and gestion\r\n","  data_temp = prediction_innovant_gestion(data.drop([\"art_content_clean_without_lem\"],axis = 1),path_load)\r\n","  data = data.merge(data_temp, on = \"art_id\")\r\n","  print(\"Phase 2\")\r\n","  # We predict the new documents\r\n","  data_temp_2 = prediction_nouveau(data.query(\"prediction_innovant == 1\").query(\"prediction_gamme_gestion == 1\").drop([\"art_content_clean_without_lem\",\"prediction_innovant\",\"prediction_gamme_gestion\",\"score_innovant\",\"score_gamme_gestion\"],axis = 1))\r\n","  print(\"Phase 3\")\r\n","  # We predict the clusters\r\n","  data_temp_3 = prediction_clustering(data.query(\"prediction_innovant == 1\").query(\"prediction_gamme_gestion == 1\").drop([\"art_content_clean_without_lem\",\"prediction_innovant\",\"prediction_gamme_gestion\",\"score_innovant\",\"score_gamme_gestion\"],axis = 1),path_load)\r\n","  print(\"Contatenation\")\r\n","  # We merge all the data\r\n","  data = data.merge(data_temp_2,how = \"left\",on = \"art_id\")\r\n","  data = data.merge(data_temp_3,how = \"left\",on = \"art_id\")\r\n","\r\n","  # We select only the columns that we want\r\n","  dataout = data[[\"art_id\",\"prediction_innovant\",\"prediction_gamme_gestion\",\"score_innovant\",\"score_gamme_gestion\",\"prediction_nouveau\",\"score_nouveau\",\"prediction_theme\"]]\r\n","  dataout.rename(columns = { \"prediction_nouveau\" : \"nouveau\",\"prediction_theme\" : \"theme\"}, inplace = True)\r\n","\r\n","  # We change the name of the clusters\r\n","  dict_label = pickle.load(open(path_load + \"dict_label.pkl\", 'rb'))\r\n","  dataout[\"theme\"] = dataout[\"theme\"].map(dict_label)\r\n","\r\n","  return dataout"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yanczv0AR16w"},"source":["# We run the function"]},{"cell_type":"code","metadata":{"id":"rzH5Tw0t4Wgb"},"source":["# We load data\r\n","data = pd.read_json(\r\n","    \"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Input/g3_BOW_v1.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dq3xI0Z80jZz","executionInfo":{"status":"ok","timestamp":1610795328610,"user_tz":-60,"elapsed":221401,"user":{"displayName":"Johann Jérémy","photoUrl":"","userId":"04957728469021807651"}},"outputId":"4bedcc8d-da94-4d2b-f1f8-7fc67803fd7b"},"source":["# We make all prediction\r\n","output = all_prediction(data,\"/content/drive/MyDrive/G5 Inter-Promo 2021/Ressources/test/\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Phase 1\n","Phase 2\n","Phase 3\n","Contatenation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sQ7FgbtlQjlw","executionInfo":{"status":"ok","timestamp":1610980348777,"user_tz":-60,"elapsed":4470,"user":{"displayName":"Johann Jérémy","photoUrl":"","userId":"04957728469021807651"}}},"source":["# We save the data\r\n","output = pd.read_json(\"/content/drive/MyDrive/G5 Inter-Promo 2021/Données/Output/Global/Global_V2.json\")"],"execution_count":4,"outputs":[]}]}